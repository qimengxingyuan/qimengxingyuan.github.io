<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>祈梦星缘的Blog</title>
  
  <subtitle>Welcome</subtitle>
  <link href="https://yhzhao.cn/atom.xml" rel="self"/>
  
  <link href="https://yhzhao.cn/"/>
  <updated>2021-09-14T16:38:00.620Z</updated>
  <id>https://yhzhao.cn/</id>
  
  <author>
    <name>zhaoyonghui</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Search of an Understandable Consensus Algorithm</title>
    <link href="https://yhzhao.cn/articles/24081.html"/>
    <id>https://yhzhao.cn/articles/24081.html</id>
    <published>2021-09-15T00:35:26.000Z</published>
    <updated>2021-09-14T16:38:00.620Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文为raft论文 《Search of an Understandable Consensus Algorithm 》翻译</p><p>原文传送门：🔗<a href="https://ramcloud.atlassian.net/wiki/download/attachments/6586375/raft.pdf">英文版地址</a></p><p>翻译来自于GitHub项目：<a href="https://github.com/maemual/raft-zh_cn">raft-zh_cn</a></p><h1 id="寻找一种易于理解的一致性算法（扩展版）"><a href="#寻找一种易于理解的一致性算法（扩展版）" class="headerlink" title="寻找一种易于理解的一致性算法（扩展版）"></a>寻找一种易于理解的一致性算法（扩展版）</h1><ul><li><a href="#%E5%AF%BB%E6%89%BE%E4%B8%80%E7%A7%8D%E6%98%93%E4%BA%8E%E7%90%86%E8%A7%A3%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E6%89%A9%E5%B1%95%E7%89%88">寻找一种易于理解的一致性算法（扩展版）</a><ul><li><a href="#%E6%91%98%E8%A6%81">摘要</a></li><li><a href="#1-%E4%BB%8B%E7%BB%8D">1 介绍</a></li><li><a href="#2-%E5%A4%8D%E5%88%B6%E7%8A%B6%E6%80%81%E6%9C%BA">2 复制状态机</a></li><li><a href="#3-paxos-%E7%AE%97%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98">3 Paxos 算法的问题</a></li><li><a href="#4-%E4%B8%BA%E4%BA%86%E5%8F%AF%E7%90%86%E8%A7%A3%E6%80%A7%E7%9A%84%E8%AE%BE%E8%AE%A1">4 为了可理解性的设计</a></li><li><a href="#5-raft-%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95">5 Raft 一致性算法</a><ul><li><a href="#51-raft-%E5%9F%BA%E7%A1%80">5.1 Raft 基础</a></li><li><a href="#52-%E9%A2%86%E5%AF%BC%E4%BA%BA%E9%80%89%E4%B8%BE">5.2 领导人选举</a></li><li><a href="#53-%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6">5.3 日志复制</a></li><li><a href="#54-%E5%AE%89%E5%85%A8%E6%80%A7">5.4 安全性</a><ul><li><a href="#541-%E9%80%89%E4%B8%BE%E9%99%90%E5%88%B6">5.4.1 选举限制</a></li><li><a href="#542-%E6%8F%90%E4%BA%A4%E4%B9%8B%E5%89%8D%E4%BB%BB%E6%9C%9F%E5%86%85%E7%9A%84%E6%97%A5%E5%BF%97%E6%9D%A1%E7%9B%AE">5.4.2 提交之前任期内的日志条目</a></li><li><a href="#543-%E5%AE%89%E5%85%A8%E6%80%A7%E8%AE%BA%E8%AF%81">5.4.3 安全性论证</a></li></ul></li><li><a href="#55-%E8%B7%9F%E9%9A%8F%E8%80%85%E5%92%8C%E5%80%99%E9%80%89%E4%BA%BA%E5%B4%A9%E6%BA%83">5.5 跟随者和候选人崩溃</a></li><li><a href="#56-%E6%97%B6%E9%97%B4%E5%92%8C%E5%8F%AF%E7%94%A8%E6%80%A7">5.6 时间和可用性</a></li></ul></li><li><a href="#6-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E5%8F%98%E5%8C%96">6 集群成员变化</a></li><li><a href="#7-%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9">7 日志压缩</a></li><li><a href="#8-%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BA%A4%E4%BA%92">8 客户端交互</a></li><li><a href="#9-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E5%92%8C%E8%AF%84%E4%BC%B0">9 算法实现和评估</a><ul><li><a href="#91-%E5%8F%AF%E7%90%86%E8%A7%A3%E6%80%A7">9.1 可理解性</a></li><li><a href="#92-%E6%AD%A3%E7%A1%AE%E6%80%A7">9.2 正确性</a></li><li><a href="#93-%E6%80%A7%E8%83%BD">9.3 性能</a></li></ul></li><li><a href="#10-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">10 相关工作</a></li><li><a href="#11-%E7%BB%93%E8%AE%BA">11 结论</a></li><li><a href="#12-%E6%84%9F%E8%B0%A2">12 感谢</a></li><li><a href="#%E5%8F%82%E8%80%83">参考</a></li></ul></li></ul><h2 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h2><p>Raft 是一种为了管理复制日志的一致性算法。它提供了和 Paxos 算法相同的功能和性能，但是它的算法结构和 Paxos 不同，使得 Raft 算法更加容易理解并且更容易构建实际的系统。为了提升可理解性，Raft 将一致性算法分解成了几个关键模块，例如领导人选举、日志复制和安全性。同时它通过实施一个更强的一致性来减少需要考虑的状态的数量。从一个用户研究的结果可以证明，对于学生而言，Raft 算法比 Paxos 算法更加容易学习。Raft 算法还包括一个新的机制来允许集群成员的动态改变，它利用重叠的大多数来保证安全性。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>一致性算法允许一组机器像一个整体一样工作，即使其中一些机器出现故障也能够继续工作下去。正因为如此，一致性算法在构建可信赖的大规模软件系统中扮演着重要的角色。在过去的 10 年里，Paxos 算法统治着一致性算法这一领域：绝大多数的实现都是基于 Paxos 或者受其影响。同时 Paxos 也成为了教学领域里讲解一致性问题时的示例。</p><p>但是不幸的是，尽管有很多工作都在尝试降低它的复杂性，但是 Paxos 算法依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。这些都导致了工业界和学术界都对 Paxos 算法感到十分头疼。</p><p>和 Paxos 算法进行过努力之后，我们开始寻找一种新的一致性算法，可以为构建实际的系统和教学提供更好的基础。与 Paxos 不同，我们的首要目标是可理解性：我们是否可以在实际系统中定义一个一致性算法，并且比 Paxos 算法更容易学习。此外，我们希望该算法方便系统构建者的直觉的发展。重要的不仅仅是算法能够工作，更重要的是能够显而易见的知道它为什么能工作。</p><p>Raft 一致性算法就是这些工作的结果。在设计 Raft 算法的时候，我们使用一些特别的技巧来提升它的可理解性，包括算法分解（Raft 主要被分成了领导人选举，日志复制和安全三个模块）和减少状态机的状态（相对于 Paxos，Raft 减少了非确定性和服务器互相处于非一致性的方式）。一份针对两所大学 43 个学生的研究表明 Raft 明显比 Paxos 算法更加容易理解。在这些学生同时学习了这两种算法之后，和 Paxos 比起来，其中 33 个学生能够回答有关于 Raft 的问题。</p><p>Raft 算法在许多方面和现有的一致性算法都很相似（主要是 Oki 和 Liskov 的 Viewstamped Replication），但是它也有一些独特的特性：</p><ul><li><strong>强领导者</strong>：和其他一致性算法相比，Raft 使用一种更强的领导能力形式。比如，日志条目只从领导者发送给其他的服务器。这种方式简化了对复制日志的管理并且使得 Raft 算法更加易于理解。</li><li><strong>领导选举</strong>：Raft 算法使用一个随机计时器来选举领导者。这种方式只是在任何一致性算法都必须实现的心跳机制上增加了一点机制。在解决冲突的时候会更加简单快捷。</li><li><strong>成员关系调整</strong>：Raft 使用一种共同一致的方法来处理集群成员变换的问题，在这种方法下，处于调整过程中的两种不同的配置集群中大多数机器会有重叠，这就使得集群在成员变换的时候依然可以继续工作。</li></ul><p>我们相信，Raft 算法不论出于教学目的还是作为实践项目的基础都是要比 Paxos 或者其他一致性算法要优异的。它比其他算法更加简单，更加容易理解；它的算法描述足以实现一个现实的系统；它有好多开源的实现并且在很多公司里使用；它的安全性已经被证明；它的效率和其他算法比起来也不相上下。</p><p>接下来，这篇论文会介绍以下内容：复制状态机问题（第 2 节），讨论 Paxos 的优点和缺点（第 3 节），讨论我们为了可理解性而采取的方法（第 4 节），阐述 Raft 一致性算法（第 5-8 节），评价 Raft 算法（第 9 节），以及一些相关的工作（第 10 节）。</p><h2 id="2-复制状态机"><a href="#2-复制状态机" class="headerlink" title="2 复制状态机"></a>2 复制状态机</h2><p>一致性算法是从复制状态机的背景下提出的（参考英文原文引用37）。在这种方法中，一组服务器上的状态机产生相同状态的副本，并且在一些机器宕掉的情况下也可以继续运行。复制状态机在分布式系统中被用于解决很多容错的问题。例如，大规模的系统中通常都有一个集群领导者，像 GFS、HDFS 和 RAMCloud，典型应用就是一个独立的复制状态机去管理领导选举和存储配置信息并且在领导人宕机的情况下也要存活下来。比如 Chubby 和 ZooKeeper。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE1.png"></p><blockquote><p>图 1 ：复制状态机的结构。一致性算法管理着来自客户端指令的复制日志。状态机从日志中处理相同顺序的相同指令，所以产生的结果也是相同的。</p></blockquote><p>复制状态机通常都是基于复制日志实现的，如图 1。每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。</p><p>保证复制日志相同就是一致性算法的工作了。在一台服务器上，一致性模块接收客户端发送来的指令然后增加到自己的日志中去。它和其他服务器上的一致性模块进行通信来保证每一个服务器上的日志最终都以相同的顺序包含相同的请求，尽管有些服务器会宕机。一旦指令被正确的复制，每一个服务器的状态机按照日志顺序处理他们，然后输出结果被返回给客户端。因此，服务器集群看起来形成一个高可靠的状态机。</p><p>实际系统中使用的一致性算法通常含有以下特性：</p><ul><li>安全性保证（绝对不会返回一个错误的结果）：在非拜占庭错误情况下，包括网络延迟、分区、丢包、冗余和乱序等错误都可以保证正确。</li><li>可用性：集群中只要有大多数的机器可运行并且能够相互通信、和客户端通信，就可以保证可用。因此，一个典型的包含 5 个节点的集群可以容忍两个节点的失败。服务器被停止就认为是失败。他们当有稳定的存储的时候可以从状态中恢复回来并重新加入集群。</li><li>不依赖时序来保证一致性：物理时钟错误或者极端的消息延迟只有在最坏情况下才会导致可用性问题。</li><li>通常情况下，一条指令可以尽可能快的在集群中大多数节点响应一轮远程过程调用时完成。小部分比较慢的节点不会影响系统整体的性能。</li></ul><h2 id="3-Paxos-算法的问题"><a href="#3-Paxos-算法的问题" class="headerlink" title="3 Paxos 算法的问题"></a>3 Paxos 算法的问题</h2><p>在过去的 10 年里，Leslie Lamport 的 Paxos 算法几乎已经成为一致性的代名词：Paxos 是在课程教学中最经常使用的算法，同时也是大多数一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，比如单条的复制日志项。我们把这一子集叫做单决策 Paxos。然后通过组合多个 Paxos 协议的实例来促进一系列决策的达成。Paxos 保证安全性和活性，同时也支持集群成员关系的变更。Paxos 的正确性已经被证明，在通常情况下也很高效。</p><p>不幸的是，Paxos 有两个明显的缺点。第一个缺点是 Paxos 算法特别的难以理解。完整的解释是出了名的不透明；通过极大的努力之后，也只有少数人成功理解了这个算法。因此，有了几次用更简单的术语来解释 Paxos 的尝试。尽管这些解释都只关注了单决策的子集问题，但依然很具有挑战性。在 2012 年 NSDI 的会议中的一次调查显示，很少有人对 Paxos 算法感到满意，甚至在经验老道的研究者中也是如此。我们自己也尝试去理解 Paxos；我们一直没能理解 Paxos 直到我们读了很多对 Paxos 的简化解释并且设计了我们自己的算法之后，这一过程花了近一年时间。</p><p>我们假设 Paxos 的不透明性来自它选择单决策问题作为它的基础。单决策 Paxos 是晦涩微妙的，它被划分成了两种没有简单直观解释和无法独立理解的情景。因此，这导致了很难建立起直观的感受为什么单决策 Paxos 算法能够工作。构成多决策 Paxos 增加了很多错综复杂的规则。我们相信，在多决策上达成一致性的问题（一份日志而不是单一的日志记录）能够被分解成其他的方式并且更加直接和明显。</p><p>Paxos算法的第二个问题就是它没有提供一个足够好的用来构建一个现实系统的基础。一个原因是还没有一种被广泛认同的多决策问题的算法。Lamport 的描述基本上都是关于单决策 Paxos 的；他简要描述了实施多决策 Paxos 的方法，但是缺乏很多细节。当然也有很多具体化 Paxos 的尝试，但是他们都互相不一样，和 Paxos 的概述也不同。例如 Chubby 这样的系统实现了一个类似于 Paxos 的算法，但是大多数的细节并没有被公开。</p><p>而且，Paxos 算法的结构也不是十分易于构建实践的系统；单决策分解也会产生其他的结果。例如，独立的选择一组日志条目然后合并成一个序列化的日志并没有带来太多的好处，仅仅增加了不少复杂性。围绕着日志来设计一个系统是更加简单高效的；新日志条目以严格限制的顺序增添到日志中去。另一个问题是，Paxos 使用了一种对等的点对点的方式作为它的核心（尽管它最终提议了一种弱领导人的方法来优化性能）。在只有一个决策会被制定的简化世界中是很有意义的，但是很少有现实的系统使用这种方式。如果有一系列的决策需要被制定，首先选择一个领导人，然后让他去协调所有的决议，会更加简单快速。</p><p>因此，实际的系统中很少有和 Paxos 相似的实践。每一种实现都是从 Paxos 开始研究，然后发现很多实现上的难题，再然后开发了一种和 Paxos 明显不一样的结构。这样是非常费时和容易出错的，并且理解 Paxos 的难度使得这个问题更加糟糕。Paxos 算法在理论上被证明是正确可行的，但是现实的系统和 Paxos 差别是如此的大，以至于这些证明没有什么太大的价值。下面来自 Chubby 实现非常典型：</p><blockquote><p>在Paxos算法描述和实现现实系统中间有着巨大的鸿沟。最终的系统建立在一种没有经过证明的算法之上。</p></blockquote><p>由于以上问题，我们认为 Paxos 算法既没有提供一个良好的基础给实践的系统，也没有给教学很好的帮助。基于一致性问题在大规模软件系统中的重要性，我们决定看看我们是否可以设计一个拥有更好特性的替代 Paxos 的一致性算法。Raft 算法就是这次实验的结果。</p><h2 id="4-为了可理解性的设计"><a href="#4-为了可理解性的设计" class="headerlink" title="4 为了可理解性的设计"></a>4 为了可理解性的设计</h2><p>设计 Raft 算法我们有几个初衷：它必须提供一个完整的实际的系统实现基础，这样才能大大减少开发者的工作；它必须在任何情况下都是安全的并且在大多数的情况下都是可用的；并且它的大部分操作必须是高效的。但是我们最重要也是最大的挑战是可理解性。它必须保证对于普遍的人群都可以十分容易的去理解。另外，它必须能够让人形成直观的认识，这样系统的构建者才能够在现实中进行必然的扩展。</p><p>在设计 Raft 算法的时候，有很多的点需要我们在各种备选方案中进行选择。在这种情况下，我们评估备选方案基于可理解性原则：解释各个备选方案有多大的难度（例如，Raft 的状态空间有多复杂，是否有微妙的暗示）？对于一个读者而言，完全理解这个方案和暗示是否容易？</p><p>我们意识到对这种可理解性分析上具有高度的主观性；尽管如此，我们使用了两种通常适用的技术来解决这个问题。第一个技术就是众所周知的问题分解：只要有可能，我们就将问题分解成几个相对独立的，可被解决的、可解释的和可理解的子问题。例如，Raft 算法被我们分成领导人选举，日志复制，安全性和角色改变几个部分。</p><p>我们使用的第二个方法是通过减少状态的数量来简化需要考虑的状态空间，使得系统更加连贯并且在可能的时候消除不确定性。特别的，所有的日志是不允许有空洞的，并且 Raft 限制了日志之间变成不一致状态的可能。尽管在大多数情况下我们都试图去消除不确定性，但是也有一些情况下不确定性可以提升可理解性。尤其是，随机化方法增加了不确定性，但是他们有利于减少状态空间数量，通过处理所有可能选择时使用相似的方法。我们使用随机化去简化 Raft 中领导人选举算法。</p><h2 id="5-Raft-一致性算法"><a href="#5-Raft-一致性算法" class="headerlink" title="5 Raft 一致性算法"></a>5 Raft 一致性算法</h2><p>Raft 是一种用来管理章节 2 中描述的复制日志的算法。图 2 为了参考之用，总结这个算法的简略版本，图 3 列举了这个算法的一些关键特性。图中的这些元素会在剩下的章节逐一介绍。</p><p>Raft 通过选举一个高贵的领导人，然后给予他全部的管理复制日志的责任来实现一致性。领导人从客户端接收日志条目(log entries)，把日志条目复制到其他服务器上，并且当保证安全性的时候告诉其他的服务器应用日志条目到他们的状态机中。拥有一个领导人大大简化了对复制日志的管理。例如，领导人可以决定新的日志条目需要放在日志中的什么位置而不需要和其他服务器商议，并且数据都从领导人流向其他服务器。一个领导人可能会宕机，或者和其他服务器失去连接，在这种情况下一个新的领导人会被选举出来。</p><p>通过领导人的方式，Raft 将一致性问题分解成了三个相对独立的子问题，这些问题会在接下来的子章节中进行讨论：</p><ul><li><strong>领导选举</strong>：当现存的领导人宕机的时候, 一个新的领导人需要被选举出来（章节 5.2）</li><li><strong>日志复制</strong>：领导人必须从客户端接收日志条目(log entries)然后复制到集群中的其他节点，并且强制要求其他节点的日志保持和自己相同。</li><li><strong>安全性</strong>：在 Raft 中安全性的关键是在图 3 中展示的状态机安全：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其他服务器节点不能在同一个日志索引位置应用一个不同的指令。章节 5.4 阐述了 Raft 算法是如何保证这个特性的；这个解决方案涉及到一个额外的选举机制（5.2 节）上的限制。</li></ul><p>在展示一致性算法之后，这一章节会讨论可用性的一些问题和计时在系统的作用。</p><p><strong>状态</strong>：</p><p>所有服务器上的持久性状态<br>(在响应RPC请求之前 已经更新到了稳定的存储设备)<br>| 参数 | 解释 |<br>| — | — |<br>| currentTerm | 服务器已知最新的任期（在服务器首次启动的时候初始化为0，单调递增）|<br>| votedFor | 当前任期内收到选票的候选者id 如果没有投给任何候选者 则为空|<br>| log[] | 日志条目;每个条目包含了用于状态机的命令，以及领导者接收到该条目时的任期（第一个索引为1） |</p><p>所有服务器上的易失性状态<br>| 参数 | 解释 |<br>| — | — |<br>| commitIndex | 已知已提交的最高的日志条目的索引（初始值为0，单调递增）|<br>| lastApplied | 已经被应用到状态机的最高的日志条目的索引（初始值为0，单调递增）|</p><p>领导者（服务器）上的易失性状态<br>(选举后已经重新初始化)<br>| 参数 | 解释 |<br>| — | — |<br>| nextIndex[] | 对于每一台服务器，发送到该服务器的下一个日志条目的索引（初始值为领导者最后的日志条目的索引+1）|<br>| matchIndex[] | 对于每一台服务器，已知的已经复制到该服务器的最高日志条目的索引（初始值为0，单调递增）|</p><p><strong>追加条目RPC</strong>：</p><p>由领导者调用 用于日志条目的复制 同时也被当做心跳使用</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>领导者的任期</td></tr><tr><td>leaderId</td><td>领导者ID 因此跟随者可以对客户端进行重定向（译者注：跟随者根据领导者id把客户端的请求重定向到领导者，比如有时客户端把请求发给了跟随者而不是领导者）</td></tr><tr><td>prevLogIndex</td><td>紧邻新日志条目之前的那个日志条目的索引</td></tr><tr><td>prevLogTerm</td><td>紧邻新日志条目之前的那个日志条目的任期</td></tr><tr><td>entries[]</td><td>需要被保存的日志条目（被当做心跳使用时，则日志条目内容为空；为了提高效率可能一次性发送多个）</td></tr><tr><td>leaderCommit</td><td>领导者的已知已提交的最高的日志条目的索引</td></tr></tbody></table><table><thead><tr><th>返回值</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>当前任期,对于领导者而言 它会更新自己的任期</td></tr><tr><td>success</td><td>结果为真 如果跟随者所含有的条目和prevLogIndex以及prevLogTerm匹配上了</td></tr></tbody></table><p>接收者的实现：</p><ol><li>返回假 如果领导者的任期 小于 接收者的当前任期（译者注：这里的接收者是指跟随者或者候选者）（5.1 节）</li><li>返回假 如果接收者日志中没有包含这样一个条目 即该条目的任期在prevLogIndex上能和prevLogTerm匹配上<br> （译者注：在接收者日志中 如果能找到一个和prevLogIndex以及prevLogTerm一样的索引和任期的日志条目 则继续执行下面的步骤 否则返回假）（5.3 节）</li><li>如果一个已经存在的条目和新条目（译者注：即刚刚接收到的日志条目）发生了冲突（因为索引相同，任期不同），那么就删除这个已经存在的条目以及它之后的所有条目 （5.3 节）</li><li>追加日志中尚未存在的任何新条目</li><li>如果领导者的已知已经提交的最高的日志条目的索引leaderCommit 大于 接收者的已知已经提交的最高的日志条目的索引commitIndex<br>则把 接收者的已知已经提交的最高的日志条目的索引commitIndex 重置为 领导者的已知已经提交的最高的日志条目的索引leaderCommit 或者是 上一个新条目的索引 取两者的最小值</li></ol><p><strong>请求投票 RPC</strong>：</p><p>由候选人负责调用用来征集选票（5.2 节）</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>候选人的任期号</td></tr><tr><td>candidateId</td><td>请求选票的候选人的 Id</td></tr><tr><td>lastLogIndex</td><td>候选人的最后日志条目的索引值</td></tr><tr><td>lastLogTerm</td><td>候选人最后日志条目的任期号</td></tr></tbody></table><table><thead><tr><th>返回值</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>当前任期号，以便于候选人去更新自己的任期号</td></tr><tr><td>voteGranted</td><td>候选人赢得了此张选票时为真</td></tr></tbody></table><p>接收者实现：</p><ol><li>如果<code>term &lt; currentTerm</code>返回 false （5.2 节）</li><li>如果 votedFor 为空或者为 candidateId，并且候选人的日志至少和自己一样新，那么就投票给他（5.2 节，5.4 节）</li></ol><p><strong>所有服务器需遵守的规则</strong>：</p><p>所有服务器：</p><ul><li>如果<code>commitIndex &gt; lastApplied</code>，那么就 lastApplied 加一，并把<code>log[lastApplied]</code>应用到状态机中（5.3 节）</li><li>如果接收到的 RPC 请求或响应中，任期号<code>T &gt; currentTerm</code>，那么就令 currentTerm 等于 T，并切换状态为跟随者（5.1 节）</li></ul><p>跟随者（5.2 节）：</p><ul><li>响应来自候选人和领导者的请求</li><li>如果在超过选举超时时间的情况之前没有收到<strong>当前领导人</strong>（即该领导人的任期需与这个跟随者的当前任期相同）的心跳/附加日志，或者是给某个候选人投了票，就自己变成候选人</li></ul><p>候选人（5.2 节）：</p><ul><li>在转变成候选人后就立即开始选举过程<ul><li>自增当前的任期号（currentTerm）</li><li>给自己投票</li><li>重置选举超时计时器</li><li>发送请求投票的 RPC 给其他所有服务器</li></ul></li><li>如果接收到大多数服务器的选票，那么就变成领导人</li><li>如果接收到来自新的领导人的附加日志 RPC，转变成跟随者</li><li>如果选举过程超时，再次发起一轮选举</li></ul><p>领导人：</p><ul><li>一旦成为领导人：发送空的附加日志 RPC（心跳）给其他所有的服务器；在一定的空余时间之后不停的重复发送，以阻止跟随者超时（5.2 节）</li><li> 如果接收到来自客户端的请求：附加条目到本地日志中，在条目被应用到状态机后响应客户端（5.3 节）</li><li>如果对于一个跟随者，最后日志条目的索引值大于等于 nextIndex，那么：发送从 nextIndex 开始的所有日志条目：<ul><li>如果成功：更新相应跟随者的 nextIndex 和 matchIndex</li><li>如果因为日志不一致而失败，减少 nextIndex 重试</li></ul></li><li>假设存在大于 <code>commitIndex</code> 的 <code>N</code>，使得大多数的 <code>matchIndex[i] ≥ N</code> 成立，且 <code>log[N].term == currentTerm</code> 成立，则令 <code>commitIndex</code> 等于 <code>N</code> （5.3 和 5.4 节）</li></ul><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE2.png" alt="图 2 "></p><blockquote><p>图 2：一个关于 Raft 一致性算法的浓缩总结（不包括成员变换和日志压缩）。</p></blockquote><table><thead><tr><th>特性</th><th>解释</th></tr></thead><tbody><tr><td>选举安全特性</td><td>对于一个给定的任期号，最多只会有一个领导人被选举出来（5.2 节）</td></tr><tr><td>领导人只附加原则</td><td>领导人绝对不会删除或者覆盖自己的日志，只会增加（5.3 节）</td></tr><tr><td>日志匹配原则</td><td>如果两个日志在某一相同索引位置日志条目的任期号相同，那么我们就认为这两个日志从头到该索引位置之间的内容完全一致（5.3 节）</td></tr><tr><td>领导人完全特性</td><td>如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中（5.4 节）</td></tr><tr><td>状态机安全特性</td><td>如一服务器已将给定索引位置的日志条目应用至其状态机中，则其他任何服务器在该索引位置不会应用不同的日志条目（5.4.3 节）</td></tr></tbody></table><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE3.png" alt="图 3 "></p><blockquote><p>图 3：Raft 在任何时候都保证以上的各个特性。</p></blockquote><h3 id="5-1-Raft-基础"><a href="#5-1-Raft-基础" class="headerlink" title="5.1 Raft 基础"></a>5.1 Raft 基础</h3><p>一个 Raft 集群包含若干个服务器节点；5 个服务器节点是一个典型的例子，这允许整个系统容忍 2 个节点失效。在任何时刻，每一个服务器节点都处于这三个状态之一：领导人、跟随者或者候选人。在通常情况下，系统中只有一个领导人并且其他的节点全部都是跟随者。跟随者都是被动的：他们不会发送任何请求，只是简单的响应来自领导者或者候选人的请求。领导人处理所有的客户端请求（如果一个客户端和跟随者联系，那么跟随者会把请求重定向给领导人）。第三种状态，候选人，是用来在 5.2 节描述的选举新领导人时使用。图 4 展示了这些状态和他们之间的转换关系；这些转换关系会在接下来进行讨论。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE4.png" alt="图 4 "></p><blockquote><p>图 4：服务器状态。跟随者只响应来自其他服务器的请求。如果跟随者接收不到消息，那么他就会变成候选人并发起一次选举。获得集群中大多数选票的候选人将成为领导者。在一个任期内，领导人一直都会是领导人直到自己宕机了。</p></blockquote><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE5.png" alt="图 5"></p><blockquote><p>图 5：时间被划分成一个个的任期，每个任期开始都是一次选举。在选举成功后，领导人会管理整个集群直到任期结束。有时候选举会失败，那么这个任期就会没有领导人而结束。任期之间的切换可以在不同的时间不同的服务器上观察到。</p></blockquote><p>Raft 把时间分割成任意长度的<strong>任期</strong>，如图 5。任期用连续的整数标记。每一段任期从一次<strong>选举</strong>开始，就像章节 5.2 描述的一样，一个或者多个候选人尝试成为领导者。如果一个候选人赢得选举，然后他就在接下来的任期内充当领导人的职责。在某些情况下，一次选举过程会造成选票的瓜分。在这种情况下，这一任期会以没有领导人结束；一个新的任期（和一次新的选举）会很快重新开始。Raft 保证了在一个给定的任期内，最多只有一个领导者。</p><p>不同的服务器节点可能多次观察到任期之间的转换，但在某些情况下，一个节点也可能观察不到任何一次选举或者整个任期全程。任期在 Raft 算法中充当逻辑时钟的作用，这会允许服务器节点查明一些过期的信息比如陈旧的领导者。每一个节点存储一个当前任期号，这一编号在整个时期内单调的增长。当服务器之间通信的时候会交换当前任期号；如果一个服务器的当前任期号比其他人小，那么他会更新自己的编号到较大的编号值。如果一个候选人或者领导者发现自己的任期号过期了，那么他会立即恢复成跟随者状态。如果一个节点接收到一个包含过期的任期号的请求，那么他会直接拒绝这个请求。</p><p>Raft 算法中服务器节点之间通信使用远程过程调用（RPCs），并且基本的一致性算法只需要两种类型的 RPCs。请求投票（RequestVote） RPCs 由候选人在选举期间发起（章节  5.2），然后附加条目（AppendEntries）RPCs 由领导人发起，用来复制日志和提供一种心跳机制（章节 5.3）。第 7 节为了在服务器之间传输快照增加了第三种 RPC。当服务器没有及时的收到 RPC 的响应时，会进行重试， 并且他们能够并行的发起 RPCs 来获得最佳的性能。</p><h3 id="5-2-领导人选举"><a href="#5-2-领导人选举" class="headerlink" title="5.2 领导人选举"></a>5.2 领导人选举</h3><p>Raft 使用一种心跳机制来触发领导人选举。当服务器程序启动时，他们都是跟随者身份。一个服务器节点继续保持着跟随者状态只要他从领导人或者候选者处接收到有效的 RPCs。领导者周期性的向所有跟随者发送心跳包（即不包含日志项内容的附加日志项 RPCs）来维持自己的权威。如果一个跟随者在一段时间里没有接收到任何消息，也就是<strong>选举超时</strong>，那么他就会认为系统中没有可用的领导者,并且发起选举以选出新的领导者。</p><p>要开始一次选举过程，跟随者先要增加自己的当前任期号并且转换到候选人状态。然后他会并行的向集群中的其他服务器节点发送请求投票的 RPCs 来给自己投票。候选人会继续保持着当前状态直到以下三件事情之一发生：(a) 他自己赢得了这次的选举，(b) 其他的服务器成为领导者，(c) 一段时间之后没有任何一个获胜的人。这些结果会分别的在下面的段落里进行讨论。</p><p>当一个候选人从整个集群的大多数服务器节点获得了针对同一个任期号的选票，那么他就赢得了这次选举并成为领导人。每一个服务器最多会对一个任期号投出一张选票，按照先来先服务的原则（注意：5.4 节在投票上增加了一点额外的限制）。要求大多数选票的规则确保了最多只会有一个候选人赢得此次选举（图 3 中的选举安全性）。一旦候选人赢得选举，他就立即成为领导人。然后他会向其他的服务器发送心跳消息来建立自己的权威并且阻止新的领导人的产生。</p><p>在等待投票的时候，候选人可能会从其他的服务器接收到声明它是领导人的附加日志项 RPC。如果这个领导人的任期号（包含在此次的 RPC中）不小于候选人当前的任期号，那么候选人会承认领导人合法并回到跟随者状态。 如果此次 RPC 中的任期号比自己小，那么候选人就会拒绝这次的 RPC 并且继续保持候选人状态。</p><p>第三种可能的结果是候选人既没有赢得选举也没有输：如果有多个跟随者同时成为候选人，那么选票可能会被瓜分以至于没有候选人可以赢得大多数人的支持。当这种情况发生的时候，每一个候选人都会超时，然后通过增加当前任期号来开始一轮新的选举。然而，没有其他机制的话，选票可能会被无限的重复瓜分。</p><p>Raft 算法使用随机选举超时时间的方法来确保很少会发生选票瓜分的情况，就算发生也能很快的解决。为了阻止选票起初就被瓜分，选举超时时间是从一个固定的区间（例如 150-300 毫秒）随机选择。这样可以把服务器都分散开以至于在大多数情况下只有一个服务器会选举超时；然后他赢得选举并在其他服务器超时之前发送心跳包。同样的机制被用在选票瓜分的情况下。每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，然后在超时时间内等待投票的结果；这样减少了在新的选举中另外的选票瓜分的可能性。9.3 节展示了这种方案能够快速的选出一个领导人。</p><p>领导人选举这个例子，体现了可理解性原则是如何指导我们进行方案设计的。起初我们计划使用一种排名系统：每一个候选人都被赋予一个唯一的排名，供候选人之间竞争时进行选择。如果一个候选人发现另一个候选人拥有更高的排名，那么他就会回到跟随者状态，这样高排名的候选人能够更加容易的赢得下一次选举。但是我们发现这种方法在可用性方面会有一点问题（如果高排名的服务器宕机了，那么低排名的服务器可能会超时并再次进入候选人状态。而且如果这个行为发生得足够快，则可能会导致整个选举过程都被重置掉）。我们针对算法进行了多次调整，但是每次调整之后都会有新的问题。最终我们认为随机重试的方法是更加明显和易于理解的。</p><h3 id="5-3-日志复制"><a href="#5-3-日志复制" class="headerlink" title="5.3 日志复制"></a>5.3 日志复制</h3><p>一旦一个领导人被选举出来，他就开始为客户端提供服务。客户端的每一个请求都包含一条被复制状态机执行的指令。领导人把这条指令作为一条新的日志条目附加到日志中去，然后并行的发起附加条目 RPCs 给其他的服务器，让他们复制这条日志条目。当这条日志条目被安全的复制（下面会介绍），领导人会应用这条日志条目到它的状态机中然后把执行的结果返回给客户端。如果跟随者崩溃或者运行缓慢，再或者网络丢包，领导人会不断的重复尝试附加日志条目 RPCs （尽管已经回复了客户端）直到所有的跟随者都最终存储了所有的日志条目。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE6.png" alt="图 6"></p><blockquote><p>图 6：日志由有序序号标记的条目组成。每个条目都包含创建时的任期号（图中框中的数字），和一个状态机需要执行的指令。一个条目当可以安全的被应用到状态机中去的时候，就认为是可以提交了。</p></blockquote><p>日志以图 6 展示的方式组织。每一个日志条目存储一条状态机指令和从领导人收到这条指令时的任期号。日志中的任期号用来检查是否出现不一致的情况，同时也用来保证图 3 中的某些性质。每一条日志条目同时也都有一个整数索引值来表明它在日志中的位置。</p><p>领导人来决定什么时候把日志条目应用到状态机中是安全的；这种日志条目被称为<strong>已提交</strong>。Raft 算法保证所有已提交的日志条目都是持久化的并且最终会被所有可用的状态机执行。在领导人将创建的日志条目复制到大多数的服务器上的时候，日志条目就会被提交（例如在图 6 中的条目 7）。同时，领导人的日志中之前的所有日志条目也都会被提交，包括由其他领导人创建的条目。5.4 节会讨论某些当在领导人改变之后应用这条规则的隐晦内容，同时他也展示了这种提交的定义是安全的。领导人跟踪了最大的将会被提交的日志项的索引，并且索引值会被包含在未来的所有附加日志 RPCs （包括心跳包），这样其他的服务器才能最终知道领导人的提交位置。一旦跟随者知道一条日志条目已经被提交，那么他也会将这个日志条目应用到本地的状态机中（按照日志的顺序）。</p><p>我们设计了 Raft 的日志机制来维护一个不同服务器的日志之间的高层次的一致性。这么做不仅简化了系统的行为也使得更加可预计，同时他也是安全性保证的一个重要组件。Raft 维护着以下的特性，这些同时也组成了图 3 中的日志匹配特性：</p><ul><li>如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们存储了相同的指令。</li><li>如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也全部相同。</li></ul><p>第一个特性来自这样的一个事实，领导人最多在一个任期里在指定的一个日志索引位置创建一条日志条目，同时日志条目在日志中的位置也从来不会改变。第二个特性由附加日志 RPC 的一个简单的一致性检查所保证。在发送附加日志 RPC 的时候，领导人会把新的日志条目紧接着之前的条目的索引位置和任期号包含在里面。如果跟随者在它的日志中找不到包含相同索引位置和任期号的条目，那么他就会拒绝接收新的日志条目。一致性检查就像一个归纳步骤：一开始空的日志状态肯定是满足日志匹配特性的，然后一致性检查保护了日志匹配特性当日志扩展的时候。因此，每当附加日志 RPC 返回成功时，领导人就知道跟随者的日志一定是和自己相同的了。</p><p>在正常的操作中，领导人和跟随者的日志保持一致性，所以附加日志 RPC 的一致性检查从来不会失败。然而，领导人崩溃的情况会使得日志处于不一致的状态（老的领导人可能还没有完全复制所有的日志条目）。这种不一致问题会在领导人和跟随者的一系列崩溃下加剧。图 7 展示了跟随者的日志可能和新的领导人不同的方式。跟随者可能会丢失一些在新的领导人中有的日志条目，他也可能拥有一些领导人没有的日志条目，或者两者都发生。丢失或者多出日志条目可能会持续多个任期。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE7.png" alt="图 7"></p><blockquote><p>图 7：当一个领导人成功当选时，跟随者可能是任何情况（a-f）。每一个盒子表示是一个日志条目；里面的数字表示任期号。跟随者可能会缺少一些日志条目（a-b），可能会有一些未被提交的日志条目（c-d），或者两种情况都存在（e-f）。例如，场景 f 可能会这样发生，某服务器在任期 2 的时候是领导人，已附加了一些日志条目到自己的日志中，但在提交之前就崩溃了；很快这个机器就被重启了，在任期 3 重新被选为领导人，并且又增加了一些日志条目到自己的日志中；在任期 2 和任期 3 的日志被提交之前，这个服务器又宕机了，并且在接下来的几个任期里一直处于宕机状态。</p></blockquote><p>在 Raft 算法中，领导人处理不一致是通过强制跟随者直接复制自己的日志来解决了。这意味着在跟随者中的冲突的日志条目会被领导人的日志覆盖。5.4 节会阐述如何通过增加一些限制来使得这样的操作是安全的。</p><p>要使得跟随者的日志进入和自己一致的状态，领导人必须找到最后两者达成一致的地方，然后删除从那个点之后的所有日志条目，发送自己的日志给跟随者。所有的这些操作都在进行附加日志 RPCs 的一致性检查时完成。领导人针对每一个跟随者维护了一个 <strong>nextIndex</strong>，这表示下一个需要发送给跟随者的日志条目的索引地址。当一个领导人刚获得权力的时候，他初始化所有的 nextIndex 值为自己的最后一条日志的 index 加 1（图 7 中的 11）。如果一个跟随者的日志和领导人不一致，那么在下一次的附加日志 RPC 时的一致性检查就会失败。在被跟随者拒绝之后，领导人就会减小 nextIndex 值并进行重试。最终 nextIndex 会在某个位置使得领导人和跟随者的日志达成一致。当这种情况发生，附加日志 RPC 就会成功，这时就会把跟随者冲突的日志条目全部删除并且加上领导人的日志。一旦附加日志 RPC 成功，那么跟随者的日志就会和领导人保持一致，并且在接下来的任期里一直继续保持。</p><p>如果需要的话，算法可以通过减少被拒绝的附加日志 RPCs 的次数来优化。例如，当附加日志 RPC 的请求被拒绝的时候，跟随者可以(返回)冲突条目的任期号和该任期号对应的最小索引地址。借助这些信息，领导人可以减小 nextIndex 一次性越过该冲突任期的所有日志条目；这样就变成每个任期需要一次附加条目 RPC 而不是每个条目一次。在实践中，我们十分怀疑这种优化是否是必要的，因为失败是很少发生的并且也不大可能会有这么多不一致的日志。</p><p>通过这种机制，领导人在获得权力的时候就不需要任何特殊的操作来恢复一致性。他只需要进行正常的操作，然后日志就能自动的在回复附加日志 RPC 的一致性检查失败的时候自动趋于一致。领导人从来不会覆盖或者删除自己的日志（图 3 的领导人只附加特性）。</p><p>日志复制机制展示出了第 2 节中形容的一致性特性：Raft 能够接受，复制并应用新的日志条目只要大部分的机器是工作的；在通常的情况下，新的日志条目可以在一次 RPC 中被复制给集群中的大多数机器；并且单个的缓慢的跟随者不会影响整体的性能。</p><h3 id="5-4-安全性"><a href="#5-4-安全性" class="headerlink" title="5.4 安全性"></a>5.4 安全性</h3><p>前面的章节里描述了 Raft 算法是如何选举和复制日志的。然而，到目前为止描述的机制并不能充分的保证每一个状态机会按照相同的顺序执行相同的指令。例如，一个跟随者可能会进入不可用状态同时领导人已经提交了若干的日志条目，然后这个跟随者可能会被选举为领导人并且覆盖这些日志条目；因此，不同的状态机可能会执行不同的指令序列。</p><p>这一节通过在领导选举的时候增加一些限制来完善 Raft 算法。这一限制保证了任何的领导人对于给定的任期号，都拥有了之前任期的所有被提交的日志条目（图 3 中的领导人完整特性）。增加这一选举时的限制，我们对于提交时的规则也更加清晰。最终，我们将展示对于领导人完整特性的简要证明，并且说明领导人完整性特性是如何引导复制状态机做出正确行为的。</p><h4 id="5-4-1-选举限制"><a href="#5-4-1-选举限制" class="headerlink" title="5.4.1 选举限制"></a>5.4.1 选举限制</h4><p>在任何基于领导人的一致性算法中，领导人都必须存储所有已经提交的日志条目。在某些一致性算法中，例如 Viewstamped Replication，某个节点即使是一开始并没有包含所有已经提交的日志条目，它也能被选为领导者。这些算法都包含一些额外的机制来识别丢失的日志条目并把他们传送给新的领导人，要么是在选举阶段要么在之后很快进行。不幸的是，这种方法会导致相当大的额外的机制和复杂性。Raft 使用了一种更加简单的方法，它可以保证所有之前的任期号中已经提交的日志条目在选举的时候都会出现在新的领导人中，不需要传送这些日志条目给领导人。这意味着日志条目的传送是单向的，只从领导人传给跟随者，并且领导人从不会覆盖自身本地日志中已经存在的条目。</p><p>Raft 使用投票的方式来阻止一个候选人赢得选举除非这个候选人包含了所有已经提交的日志条目。候选人为了赢得选举必须联系集群中的大部分节点，这意味着每一个已经提交的日志条目在这些服务器节点中肯定存在于至少一个节点上。如果候选人的日志至少和大多数的服务器节点一样新（这个新的定义会在下面讨论），那么他一定持有了所有已经提交的日志条目。请求投票 RPC 实现了这样的限制：RPC 中包含了候选人的日志信息，然后投票人会拒绝掉那些日志没有自己新的投票请求。</p><p>Raft 通过比较两份日志中最后一条日志条目的索引值和任期号定义谁的日志比较新。如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新。</p><h4 id="5-4-2-提交之前任期内的日志条目"><a href="#5-4-2-提交之前任期内的日志条目" class="headerlink" title="5.4.2 提交之前任期内的日志条目"></a>5.4.2 提交之前任期内的日志条目</h4><p>如同 5.3 节介绍的那样，领导人知道一条当前任期内的日志记录是可以被提交的，只要它被存储到了大多数的服务器上。如果一个领导人在提交日志条目之前崩溃了，未来后续的领导人会继续尝试复制这条日志记录。然而，一个领导人不能断定一个之前任期里的日志条目被保存到大多数服务器上的时候就一定已经提交了。图 8 展示了一种情况，一条已经被存储到大多数节点上的老日志条目，也依然有可能会被未来的领导人覆盖掉。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE8.png" alt="图 8"></p><blockquote><p>图 8：如图的时间序列展示了为什么领导人无法决定对老任期号的日志条目进行提交。在 (a) 中，S1 是领导者，部分的(跟随者)复制了索引位置 2 的日志条目。在 (b) 中，S1 崩溃了，然后 S5 在任期 3 里通过 S3、S4 和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引 2 处。然后到 (c)，S5 又崩溃了；S1 重新启动，选举成功，开始复制日志。在这时，来自任期 2 的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。如果 S1 在 (d) 中又崩溃了，S5 可以重新被选举成功（通过来自 S2，S3 和 S4 的选票），然后覆盖了他们在索引 2 处的日志。反之，如果在崩溃之前，S1 把自己主导的新任期里产生的日志条目复制到了大多数机器上，就如 (e) 中那样，那么在后面任期里面这些新的日志条目就会被提交（因为 S5 就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的日志条目就会被提交。</p></blockquote><p>为了消除图 8 里描述的情况，Raft 永远不会通过计算副本数目的方式去提交一个之前任期内的日志条目。只有领导人当前任期里的日志条目通过计算副本数目可以被提交；一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。在某些情况下，领导人可以安全的知道一个老的日志条目是否已经被提交（例如，该条目是否存储到所有服务器上），但是 Raft 为了简化问题使用一种更加保守的方法。</p><p>当领导人复制之前任期里的日志时，Raft 会为所有日志保留原始的任期号, 这在提交规则上产生了额外的复杂性。在其他的一致性算法中，如果一个新的领导人要重新复制之前的任期里的日志时，它必须使用当前新的任期号。Raft 使用的方法更加容易辨别出日志，因为它可以随着时间和日志的变化对日志维护着同一个任期编号。另外，和其他的算法相比，Raft 中的新领导人只需要发送更少日志条目（其他算法中必须在他们被提交之前发送更多的冗余日志条目来为他们重新编号）。</p><h4 id="5-4-3-安全性论证"><a href="#5-4-3-安全性论证" class="headerlink" title="5.4.3 安全性论证"></a>5.4.3 安全性论证</h4><p>在给定了完整的 Raft 算法之后，我们现在可以更加精确的讨论领导人完整性特性（这一讨论基于 9.2 节的安全性证明）。我们假设领导人完全性特性是不存在的，然后我们推出矛盾来。假设任期 T 的领导人（领导人 T）在任期内提交了一条日志条目，但是这条日志条目没有被存储到未来某个任期的领导人的日志中。设大于 T 的最小任期 U 的领导人 U 没有这条日志条目。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE9.png" alt="图 9"></p><blockquote><p>图 9：如果 S1 （任期 T 的领导者）提交了一条新的日志在它的任期里，然后 S5 在之后的任期 U 里被选举为领导人，然后至少会有一个机器，如 S3，既拥有来自 S1 的日志，也给 S5 投票了。</p></blockquote><ol><li>在领导人 U 选举的时候一定没有那条被提交的日志条目（领导人从不会删除或者覆盖任何条目）。</li><li>领导人 T 复制这条日志条目给集群中的大多数节点，同时，领导人 U 从集群中的大多数节点赢得了选票。因此，至少有一个节点（投票者、选民）同时接受了来自领导人 T 的日志条目，并且给领导人 U 投票了，如图 9。这个投票者是产生这个矛盾的关键。</li><li>这个投票者必须在给领导人 U 投票之前先接受了从领导人 T 发来的已经被提交的日志条目；否则他就会拒绝来自领导人 T 的附加日志请求（因为此时他的任期号会比 T 大）。</li><li>投票者在给领导人 U 投票时依然保存有这条日志条目，因为任何中间的领导人都包含该日志条目（根据上述的假设），领导人从不会删除条目，并且跟随者只有在和领导人冲突的时候才会删除条目。</li><li>投票者把自己选票投给领导人 U 时，领导人 U 的日志必须和投票者自己一样新。这就导致了两者矛盾之一。</li><li>首先，如果投票者和领导人 U 的最后一条日志的任期号相同，那么领导人 U 的日志至少和投票者一样长，所以领导人 U 的日志一定包含所有投票者的日志。这是另一处矛盾，因为投票者包含了那条已经被提交的日志条目，但是在上述的假设里，领导人 U 是不包含的。</li><li>除此之外，领导人 U 的最后一条日志的任期号就必须比投票人大了。此外，他也比 T 大，因为投票人的最后一条日志的任期号至少和 T 一样大（他包含了来自任期 T 的已提交的日志）。创建了领导人 U 最后一条日志的之前领导人一定已经包含了那条被提交的日志（根据上述假设，领导人 U 是第一个不包含该日志条目的领导人）。所以，根据日志匹配特性，领导人 U 一定也包含那条被提交的日志，这里产生矛盾。</li><li>这里完成了矛盾。因此，所有比 T 大的领导人一定包含了所有来自 T 的已经被提交的日志。</li><li>日志匹配原则保证了未来的领导人也同时会包含被间接提交的条目，例如图 8 (e) 中的索引 2。</li></ol><p>通过领导人完全特性，我们就能证明图 3 中的状态机安全特性，即如果服务器已经在某个给定的索引值应用了日志条目到自己的状态机里，那么其他的服务器不会应用一个不一样的日志到同一个索引值上。在一个服务器应用一条日志条目到他自己的状态机中时，他的日志必须和领导人的日志，在该条目和之前的条目上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的日志的最小任期；日志完全特性保证拥有更高任期号的领导人会存储相同的日志条目，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全特性是成立的。</p><p>最后，Raft 要求服务器按照日志中索引位置顺序应用日志条目。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的日志序列集到自己的状态机中，并且是按照相同的顺序。</p><h3 id="5-5-跟随者和候选人崩溃"><a href="#5-5-跟随者和候选人崩溃" class="headerlink" title="5.5 跟随者和候选人崩溃"></a>5.5 跟随者和候选人崩溃</h3><p>到目前为止，我们都只关注了领导人崩溃的情况。跟随者和候选人崩溃后的处理方式比领导人要简单的多，并且他们的处理方式是相同的。如果跟随者或者候选人崩溃了，那么后续发送给他们的 RPCs 都会失败。Raft 中处理这种失败就是简单的通过无限的重试；如果崩溃的机器重启了，那么这些 RPC 就会完整的成功。如果一个服务器在完成了一个 RPC，但是还没有响应的时候崩溃了，那么在他重新启动之后就会再次收到同样的请求。Raft 的 RPCs 都是幂等的，所以这样重试不会造成任何问题。例如一个跟随者如果收到附加日志请求但是他已经包含了这一日志，那么他就会直接忽略这个新的请求。</p><h3 id="5-6-时间和可用性"><a href="#5-6-时间和可用性" class="headerlink" title="5.6 时间和可用性"></a>5.6 时间和可用性</h3><p>Raft 的要求之一就是安全性不能依赖时间：整个系统不能因为某些事件运行的比预期快一点或者慢一点就产生了错误的结果。但是，可用性（系统可以及时的响应客户端）不可避免的要依赖于时间。例如，如果消息交换比服务器故障间隔时间长，候选人将没有足够长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作。</p><p>领导人选举是 Raft 中对时间要求最为关键的方面。Raft 可以选举并维持一个稳定的领导人,只要系统满足下面的时间要求：</p><blockquote><p>广播时间（broadcastTime）  &lt;&lt;  选举超时时间（electionTimeout） &lt;&lt;  平均故障间隔时间（MTBF）</p></blockquote><p>在这个不等式中，广播时间指的是从一个服务器并行的发送 RPCs 给集群中的其他服务器并接收响应的平均时间；选举超时时间就是在 5.2 节中介绍的选举的超时时间限制；然后平均故障间隔时间就是对于一台服务器而言，两次故障之间的平均时间。广播时间必须比选举超时时间小一个量级，这样领导人才能够发送稳定的心跳消息来阻止跟随者开始进入选举状态；通过随机化选举超时时间的方法，这个不等式也使得选票瓜分的情况变得不可能。选举超时时间应该要比平均故障间隔时间小上几个数量级，这样整个系统才能稳定的运行。当领导人崩溃后，整个系统会大约相当于选举超时的时间里不可用；我们希望这种情况在整个系统的运行中很少出现。</p><p>广播时间和平均故障间隔时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft 的 RPCs 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒，取决于存储的技术。因此，选举超时时间可能需要在 10 毫秒到 500 毫秒之间。大多数的服务器的平均故障间隔时间都在几个月甚至更长，很容易满足时间的需求。</p><h2 id="6-集群成员变化"><a href="#6-集群成员变化" class="headerlink" title="6 集群成员变化"></a>6 集群成员变化</h2><p>到目前为止，我们都假设集群的配置（加入到一致性算法的服务器集合）是固定不变的。但是在实践中，偶尔是会改变集群的配置的，例如替换那些宕机的机器或者改变复制级别。尽管可以通过暂停整个集群，更新所有配置，然后重启整个集群的方式来实现，但是在更改的时候集群会不可用。另外，如果存在手工操作步骤，那么就会有操作失误的风险。为了避免这样的问题，我们决定自动化配置改变并且将其纳入到 Raft 一致性算法中来。</p><p>为了让配置修改机制能够安全，那么在转换的过程中不能够存在任何时间点使得两个领导人同时被选举成功在同一个任期里。不幸的是，任何服务器直接从旧的配置直接转换到新的配置的方案都是不安全的。一次性原子地转换所有服务器是不可能的，所以在转换期间整个集群存在划分成两个独立的大多数群体的可能性（见图 10）。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE10.png" alt="图 10"></p><blockquote><p>图 10：直接从一种配置转到新的配置是十分不安全的，因为各个机器可能在任何的时候进行转换。在这个例子中，集群配额从 3 台机器变成了 5 台。不幸的是，存在这样的一个时间点，两个不同的领导人在同一个任期里都可以被选举成功。一个是通过旧的配置，一个通过新的配置。</p></blockquote><p>为了保证安全性，配置更改必须使用两阶段方法。目前有很多种两阶段的实现。例如，有些系统在第一阶段停掉旧的配置所以集群就不能处理客户端请求；然后在第二阶段在启用新的配置。在 Raft 中，集群先切换到一个过渡的配置，我们称之为共同一致；一旦共同一致已经被提交了，那么系统就切换到新的配置上。共同一致是老配置和新配置的结合：</p><ul><li>日志条目被复制给集群中新、老配置的所有服务器。</li><li>新、旧配置的服务器都可以成为领导人。</li><li>达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持。</li></ul><p>共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程中依然响应客户端的请求。</p><p>集群配置在复制日志中以特殊的日志条目来存储和通信；图 11 展示了配置转换的过程。当一个领导人接收到一个改变配置从 C-old 到 C-new 的请求，他会为了共同一致存储配置（图中的 C-old,new），以前面描述的日志条目和副本的形式。一旦一个服务器将新的配置日志条目增加到它的日志中，他就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论他是否已经被提交）。这意味着领导人要使用  C-old,new 的规则来决定日志条目 C-old,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 C-old 配置也可能是 C-old,new 配置，这取决于赢得选举的候选人是否已经接收到了 C-old,new 配置。在任何情况下， C-new 配置在这一时期都不会单方面的做出决定。</p><p>一旦 C-old,new 被提交，那么无论是 C-old 还是 C-new，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 C-new 配置的日志条目并复制给集群就是安全的了。再者，每个服务器在见到新的配置的时候就会立即生效。当新的配置在 C-new 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如图 11，C-old 和 C-new 没有任何机会同时做出单方面的决定；这保证了安全性。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE11.png" alt="图 11"></p><blockquote><p>图 11：一个配置切换的时间线。虚线表示已经被创建但是还没有被提交的配置日志条目，实线表示最后被提交的配置日志条目。领导人首先创建了 C-old,new 的配置条目在自己的日志中，并提交到 C-old,new 中（C-old 的大多数和  C-new 的大多数）。然后他创建 C-new 条目并提交到 C-new 中的大多数。这样就不存在  C-new 和 C-old 可以同时做出决定的时间点。</p></blockquote><p>在关于重新配置还有三个问题需要提出。第一个问题是，新的服务器可能初始化没有存储任何的日志条目。当这些服务器以这种状态加入到集群中，那么他们需要一段时间来更新追赶，这时还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新之前使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权身份加入到集群中来（领导人复制日志给他们，但是不考虑他们是大多数）。一旦新的服务器追赶上了集群中的其他机器，重新配置可以像上面描述的一样处理。</p><p>第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 C-new 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括他自己；他复制日志但是不把他自己算作是大多数之一。当 C-new 被提交时，会发生领导人过渡，因为这时是最早新的配置可以独立工作的时间点（将总是能够在 C-new 配置下选出新的领导人）。在此之前，可能只能从 C-old 中选出领导人。</p><p>第三个问题是，移除不在 C-new 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳，所以当选举超时，他们就会进行新的选举过程。他们会发送拥有新的任期号的请求投票 RPCs，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。</p><p>为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略请求投票 RPCs。特别的，当服务器在当前最小选举超时时间内收到一个请求投票 RPC，他不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么他就不会被更大的任期号废黜。</p><h2 id="7-日志压缩"><a href="#7-日志压缩" class="headerlink" title="7 日志压缩"></a>7 日志压缩</h2><p>Raft 的日志在正常操作中不断的增长，但是在实际的系统中，日志不能无限制的增长。随着日志不断增长，他会占用越来越多的空间，花费越来越多的时间来重置。如果没有一定的机制去清除日志里积累的陈旧的信息，那么会带来可用性问题。</p><p>快照是最简单的压缩方法。在快照系统中，整个系统的状态都以快照的形式写入到稳定的持久化存储中，然后到那个时间点之前的日志全部丢弃。快照技术被使用在 Chubby 和 ZooKeeper 中，接下来的章节会介绍 Raft 中的快照技术。</p><p>增量压缩的方法，例如日志清理或者日志结构合并树，都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以实现 LSM tree 使用和快照相同的接口，但是日志清除方法就需要修改 Raft 了。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE12.png" alt="图 12"></p><blockquote><p>图 12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。</p></blockquote><p>图 12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也包含一些少量的元数据到快照中：<strong>最后被包含索引</strong>指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），<strong>最后被包含的任期</strong>指的是该条目的任期号。保留这些数据是为了支持快照后紧接着的第一个条目的附加日志请求时的一致性检查，因为这个条目需要前一日志条目的索引值和任期号。为了支持集群成员更新（第 6 节），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。</p><p>尽管通常服务器都是独立的创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器（第 6 节）将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给他们。</p><p><strong>安装快照 RPC</strong>：</p><p>由领导人调用以将快照的分块发送给跟随者。领导者总是按顺序发送分块。</p><table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>领导人的任期号</td></tr><tr><td>leaderId</td><td>领导人的 Id，以便于跟随者重定向请求</td></tr><tr><td>lastIncludedIndex</td><td>快照中包含的最后日志条目的索引值</td></tr><tr><td>lastIncludedTerm</td><td>快照中包含的最后日志条目的任期号</td></tr><tr><td>offset</td><td>分块在快照中的字节偏移量</td></tr><tr><td>data[]</td><td>从偏移量开始的快照分块的原始字节</td></tr><tr><td>done</td><td>如果这是最后一个分块则为 true</td></tr></tbody></table><table><thead><tr><th>结果</th><th>解释</th></tr></thead><tbody><tr><td>term</td><td>当前任期号（currentTerm），便于领导人更新自己</td></tr></tbody></table><p><strong>接收者实现</strong>：</p><ol><li>如果<code>term &lt; currentTerm</code>就立即回复</li><li>如果是第一个分块（offset 为 0）就创建一个新的快照</li><li>在指定偏移量写入数据</li><li>如果 done 是 false，则继续等待更多的数据</li><li>保存快照文件，丢弃具有较小索引的任何现有或部分快照</li><li>如果现存的日志条目与快照中最后包含的日志条目具有相同的索引值和任期号，则保留其后的日志条目并进行回复</li><li>丢弃整个日志</li><li>使用快照重置状态机（并加载快照的集群配置）</li></ol><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE13.png" alt="图 13 "></p><blockquote><p>图 13：一个关于安装快照的简要概述。为了便于传输，快照都是被分成分块的；每个分块都给了跟随者生命的迹象，所以跟随者可以重置选举超时计时器。</p></blockquote><p>在这种情况下领导人使用一种叫做安装快照的新的 RPC 来发送快照给太落后的跟随者；见图 13。当跟随者通过这种  RPC 接收到快照时，他必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者丢弃其整个日志；它全部被快照取代，并且可能包含与快照冲突的未提交条目。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照后面的条目仍然有效，必须保留。</p><p>这种快照的方式背离了 Raft 的强领导人原则，因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织他们的数据了。</p><p>我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。</p><p>还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，他就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。</p><p>第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。</p><h2 id="8-客户端交互"><a href="#8-客户端交互" class="headerlink" title="8 客户端交互"></a>8 客户端交互</h2><p>这一节将介绍客户端是如何和 Raft 进行交互的，包括客户端如何发现领导人和 Raft 是如何支持线性化语义的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。</p><p>Raft 中的客户端发送所有请求给领导人。当客户端启动的时候，他会随机挑选一个服务器进行通信。如果客户端第一次挑选的服务器不是领导人，那么那个服务器会拒绝客户端的请求并且提供他最近接收到的领导人的信息（附加条目请求包含了领导人的网络地址）。如果领导人已经崩溃了，那么客户端的请求就会超时；客户端之后会再次重试随机挑选服务器的过程。</p><p>我们 Raft 的目标是要实现线性化语义（每一次操作立即执行，只执行一次，在他调用和收到回复之间）。但是，如上述，Raft 是可以执行同一条命令多次的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。</p><p>只读的操作可以直接处理而不需要记录日志。但是，在不增加任何限制的情况下，这么做可能会冒着返回脏数据的风险，因为领导人响应客户端请求时可能已经被新的领导人作废了，但是他还不知道。线性化的读操作必须不能返回脏数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。首先，领导人必须有关于被提交日志的最新信息。领导人完全特性保证了领导人一定拥有所有已经被提交的日志条目，但是在他任期开始的时候，他可能不知道哪些是已经被提交的。为了知道这些信息，他需要在他的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来实现。第二，领导人在处理只读的请求之前必须检查自己是否已经被废黜了（他自己的信息已经变脏了如果一个更新的领导人被选举出来）。Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。可选的，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时间来保证安全性（假设时间误差是有界的）。</p><h2 id="9-算法实现和评估"><a href="#9-算法实现和评估" class="headerlink" title="9 算法实现和评估"></a>9 算法实现和评估</h2><p>我们已经为 RAMCloud 实现了 Raft 算法作为存储配置信息的复制状态机的一部分，并且帮助 RAMCloud 协调故障转移。这个 Raft 实现包含大约 2000 行 C++ 代码，其中不包括测试、注释和空行。这些代码是开源的。同时也有大约 25 个其他独立的第三方的基于这篇论文草稿的开源实现，针对不同的开发场景。同时，很多公司已经部署了基于 Raft 的系统。</p><p>这一节会从三个方面来评估 Raft 算法：可理解性、正确性和性能。</p><h3 id="9-1-可理解性"><a href="#9-1-可理解性" class="headerlink" title="9.1 可理解性"></a>9.1 可理解性</h3><p>为了和 Paxos 比较 Raft 算法的可理解能力，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一次学习的实验。我们分别拍了针对 Raft 和 Paxos 的视频课程，并准备了相应的小测验。Raft 的视频讲课覆盖了这篇论文的所有内容除了日志压缩；Paxos 讲课包含了足够的资料来创建一个等价的复制状态机，包括单决策 Paxos，多决策 Paxos，重新配置和一些实际系统需要的性能优化（例如领导人选举）。小测验测试一些对算法的基本理解和解释一些边角的示例。每个学生都是看完第一个视频，回答相应的测试，再看第二个视频，回答相应的测试。大约有一半的学生先进行 Paxos 部分，然后另一半先进行 Raft 部分，这是为了说明两者从第一部分的算法学习中获得的表现和经验的差异。我们计算参加人员的每一个小测验的得分来看参与者是否在 Raft 算法上更加容易理解。</p><p>我们尽可能的使得 Paxos 和 Raft 的比较更加公平。这个实验偏爱 Paxos 表现在两个方面：43 个参加者中有 15 个人在之前有一些  Paxos 的经验，并且 Paxos 的视频要长 14%。如表格 1 总结的那样，我们采取了一些措施来减轻这种潜在的偏见。我们所有的材料都可供审查。</p><table><thead><tr><th>关心</th><th>缓和偏见采取的手段</th><th>可供查看的材料</th></tr></thead><tbody><tr><td>相同的讲课质量</td><td>两者使用同一个讲师。Paxos 使用的是现在很多大学里经常使用的。Paxos 会长 14%。</td><td>视频</td></tr><tr><td>相同的测验难度</td><td>问题以难度分组，在两个测验里成对出现。</td><td>小测验</td></tr><tr><td>公平评分</td><td>使用评价量规。随机顺序打分，两个测验交替进行。</td><td>评价量规（rubric）</td></tr></tbody></table><blockquote><p>表 1：考虑到可能会存在的偏见，对于每种情况的解决方法，和相应的材料。</p></blockquote><p>参加者平均在 Raft 的测验中比 Paxos 高 4.9 分（总分 60，那么 Raft 的平均得分是 25.7，而 Paxos 是 20.8）；图 14 展示了每个参与者的得分。配置t-检验（又称student‘s t-test）表明，在 95% 的可信度下，真实的 Raft 分数分布至少比 Paxos 高 2.5 分。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE14.png" alt="图 14"></p><blockquote><p>图 14：一个散点图表示了 43 个学生在 Paxos 和 Raft 的小测验中的成绩。在对角线之上的点表示在 Raft 获得了更高分数的学生。</p></blockquote><p>我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，基于以下三个因素：他们使用的是哪个小测验，之前对 Paxos 的经验，和学习算法的顺序。模型预测，对小测验的选择会产生 12.5 分的差别。这显著的高于之前的 4.9 分，因为很多学生在之前都已经有了对于 Paxos 的经验，这相当明显的帮助 Paxos，对 Raft 就没什么太大影响了。但是奇怪的是，模型预测对于先进行 Paxos 小测验的人而言，Raft的得分低了6.3分; 虽然我们不知道为什么，这似乎在统计上是有意义的。</p><p>我们同时也在测验之后调查了参与者，他们认为哪个算法更加容易实现和解释；这个的结果在图 15 上。压倒性的结果表明 Raft 算法更加容易实现和解释（41 人中的 33个）。但是，这种自己报告的结果不如参与者的成绩更加可信，并且参与者可能因为我们的 Raft 更加易于理解的假说而产生偏见。</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE15.png" alt="图 15"></p><blockquote><p>图 15：通过一个 5 分制的问题，参与者（左边）被问哪个算法他们觉得在一个高效正确的系统里更容易实现，右边被问哪个更容易向学生解释。</p></blockquote><p>关于 Raft 用户学习有一个更加详细的讨论。</p><h3 id="9-2-正确性"><a href="#9-2-正确性" class="headerlink" title="9.2 正确性"></a>9.2 正确性</h3><p>在第 5 节，我们已经制定了正式的规范，和对一致性机制的安全性证明。这个正式规范使用 TLA+ 规范语言使图 2 中总结的信息非常清晰。它长约400行，并作为证明的主题。同时对于任何想实现 Raft 的人也是十分有用的。我们通过 TLA 证明系统非常机械的证明了日志完全特性。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明规范的类型安全）。而且，我们已经写了一个非正式的证明关于状态机安全性是完备的，并且是相当清晰的（大约 3500 个词）。</p><h3 id="9-3-性能"><a href="#9-3-性能" class="headerlink" title="9.3 性能"></a>9.3 性能</h3><p>Raft 和其他一致性算法例如 Paxos 有着差不多的性能。在性能方面，最重要的关注点是，当领导人被选举成功时，什么时候复制新的日志条目。Raft 通过很少数量的消息包（一轮从领导人到集群大多数机器的消息）就达成了这个目的。同时，进一步提升 Raft 的性能也是可行的。例如，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他一致性算法已经提出过很多性能优化方案；其中有很多也可以应用到 Raft 中来，但是我们暂时把这个问题放到未来的工作中去。</p><p>我们使用我们自己的 Raft 实现来衡量 Raft 领导人选举的性能并且回答两个问题。首先，领导人选举的过程收敛是否快速？第二，在领导人宕机之后，最小的系统宕机时间是多久？</p><p><img src="http://image.yhzhao.cn/imgraft-%E5%9B%BE16.png" alt="图 16"></p><blockquote><p>图 16：发现并替换一个已经崩溃的领导人的时间。上面的图考察了在选举超时时间上的随机化程度，下面的图考察了最小选举超时时间。每条线代表了 1000 次实验（除了 150-150 毫秒只试了 100 次），和相应的确定的选举超时时间。例如，150-155 毫秒意思是，选举超时时间从这个区间范围内随机选择并确定下来。这个实验在一个拥有 5 个节点的集群上进行，其广播时延大约是 15 毫秒。对于 9 个节点的集群，结果也差不多。</p></blockquote><p>为了衡量领导人选举，我们反复的使一个拥有五个节点的服务器集群的领导人宕机，并计算需要多久才能发现领导人已经宕机并选出一个新的领导人（见图 16）。为了构建一个最坏的场景，在每一的尝试里，服务器都有不同长度的日志，意味着有些候选人是没有成为领导人的资格的。另外，为了促成选票瓜分的情况，我们的测试脚本在终止领导人之前同步的发送了一次心跳广播（这大约和领导人在崩溃前复制一个新的日志给其他机器很像）。领导人均匀的随机的在心跳间隔里宕机，也就是最小选举超时时间的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。</p><p>图 16 中上面的图表明，只需要在选举超时时间上使用很少的随机化就可以大大避免选票被瓜分的情况。在没有随机化的情况下，在我们的测试里，选举过程往往都需要花费超过 10 秒钟由于太多的选票瓜分的情况。仅仅增加 5 毫秒的随机化时间，就大大的改善了选举过程，现在平均的宕机时间只有 287 毫秒。增加更多的随机化时间可以大大改善最坏情况：通过增加 50 毫秒的随机化时间，最坏的完成情况（1000 次尝试）只要 513 毫秒。</p><p>图 16 中下面的图显示，通过减少选举超时时间可以减少系统的宕机时间。在选举超时时间为 12-24 毫秒的情况下，只需要平均 35 毫秒就可以选举出新的领导人（最长的一次花费了 152 毫秒）。然而，进一步降低选举超时时间的话就会违反 Raft 的时间不等式需求：在选举新领导人之前，领导人就很难发送完心跳包。这会导致没有意义的领导人改变并降低了系统整体的可用性。我们建议使用更为保守的选举超时时间，比如 150-300 毫秒；这样的时间不大可能导致没有意义的领导人改变，而且依然提供不错的可用性。</p><h2 id="10-相关工作"><a href="#10-相关工作" class="headerlink" title="10 相关工作"></a>10 相关工作</h2><p>已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中：</p><ul><li>Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰。</li><li>关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。</li><li>实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 着实有着很大的差别。</li><li>Paxos 可以应用的性能优化。</li><li>Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于领导人的方法，和 Raft 有很多相似之处。</li></ul><p>Raft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用领导人选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了领导人身上。这样就可以使得算法更加容易理解。例如，在 Paxos 中，领导人选举和基本的一致性协议是正交的：领导人选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对领导人选举的独立的机制。相比较而言，Raft 就直接将领导人选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。</p><p>像 Raft 一样，VR 和 ZooKeeper 也是基于领导人的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非领导人的功能。例如，Raft 中日志条目都遵循着从领导人发送给其他人这一个方向：附加条目 RPC 是向外发送的。在 VR 中，日志条目的流动是双向的（领导人可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。</p><p>和上述我们提及的其他基于一致性的日志复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）。VR 和 ZooKeeper 都分别定义了 10 种不同的消息类型，相对的，Raft 只有 4 种消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。另外，VR 和 ZooKeeper 都在领导人改变时传输了整个日志；所以为了能够实践中使用，额外的消息类型就很必要了。</p><p>Raft 的强领导人模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有领导人的情况下可以达到很高的性能。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。</p><p>一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。我们选择使用共同一致的方法因为他对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。Lamport 的基于 α 的方法之所以没有被 Raft 选择是因为它假设在没有领导人的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较的，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。Raft 的方法同时也需要更少的额外机制来实现，和 VR、SMART 比较而言。</p><h2 id="11-结论"><a href="#11-结论" class="headerlink" title="11 结论"></a>11 结论</h2><p>算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。</p><p>在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。把可理解性作为设计的目标改变了我们设计 Raft 的方式；随着设计的进展，我们发现自己重复使用了一些技术，比如分解问题和简化状态空间。这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。</p><h2 id="12-感谢"><a href="#12-感谢" class="headerlink" title="12 感谢"></a>12 感谢</h2><p>这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazie`res，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的领导人 Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>略</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;本文为raft论文 《Search of an Understandable Consensus Algorithm 》翻译&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="论文" scheme="https://yhzhao.cn/categories/%E8%AE%BA%E6%96%87/"/>
    
    
    <category term="分布式" scheme="https://yhzhao.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    <category term="Raft" scheme="https://yhzhao.cn/tags/Raft/"/>
    
    <category term="Paxos" scheme="https://yhzhao.cn/tags/Paxos/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://yhzhao.cn/articles/16107.html"/>
    <id>https://yhzhao.cn/articles/16107.html</id>
    <published>2021-09-14T16:38:00.620Z</published>
    <updated>2021-09-14T16:38:00.620Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎光临<a href="https://yhzhao.cn/">祈梦星缘的Blog</a>! 这是本站的第一篇文章，在这篇文章中会手把手的教你搭建完成 ✅ 属于自己的blog空间。快跟随我开始吧！ </p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><p>我们的博客搭建是基于Hexo和Githup Pages来完成的。搭建完成后的资源部署到github.io上。同时可以给我们的博客来设置一个自己的访问域名。</p><h3 id="为什么使用-GitHub-Pages"><a href="#为什么使用-GitHub-Pages" class="headerlink" title="为什么使用 GitHub Pages"></a>为什么使用 GitHub Pages</h3><p>如果你想要搭建一个轻量级的个人博客服务，GitHub Pages 相较 WordPress 之类的建站服务有什么优势呢？</p><ul><li>首先他是完全免费的，相较其他的同类产品，他能替你省下一笔服务费，节约下的钱可以让你买一些其他的会员服务；</li><li>无须自己购买云服务进行搭建，只需按步骤一步步操作即可，即使你不懂他的技术细节；</li><li>支持的功能多，玩法丰富，你可以绑定你的域名、使用免费的 HTTPS、自己 DIY 网站的主题、使用他人开发好的插件等等；</li><li>当完成搭建后，你只需要专注于文章创作就可以了，其他诸如环境搭建、系统维护、文件存储的事情一概不用操心，都由 GitHub 处理</li></ul><p>当然了，作为一款免费的服务，我们也是要遵守 GitHub 官方使用建议和限制，在使用的时候项目和网站的大小不要超过 1GB，也不要过于频繁的更新网站的内容（每小时不超过 10 个版本），每个月的也要注意带宽使用上限为 100GB。</p><p>综合来看，GitHub Pages 依旧可以说是中小型博客或项目主页的最佳选项之一。</p><h3 id="一、准备Github账号和仓库"><a href="#一、准备Github账号和仓库" class="headerlink" title="一、准备Github账号和仓库"></a>一、准备Github账号和仓库</h3><p>首先你需要注册Github账号。并在个人主界面里选择创建一个新的 Repository。要注意的是仓库名称的格式形式必须是：<code>username.github.io</code>。</p><p><img src="http://image.yhzhao.cn/img20210911151737.png"></p><p>仓库创建完成之后转到Setting页面。找到 Pages 选项，选择一个 GitHub 官方提供的主题。选择完毕之后 GitHub Pages 就会自动帮你生成好网站，在他跳转的界面点击 Commit changes 按钮，网站就可以访问了。</p><p><img src="http://image.yhzhao.cn/img20210911151840.png"></p><h3 id="二、使用Hexo进行网站开发和部署"><a href="#二、使用Hexo进行网站开发和部署" class="headerlink" title="二、使用Hexo进行网站开发和部署"></a>二、使用Hexo进行网站开发和部署</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><ol><li>首先要安装Node.js, 访问国内<a href="https://nodejs.org/en/">Nodejs</a>网站。下载后安装即可</li><li>安装hexo，打开终端输入命令<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">npm</span> i hexo-cli -g<span class="token function">npm</span> config <span class="token builtin class-name">set</span> registry https://registry.npm.taobao.org // 添加国内镜像源<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>后续可以使用<code>npm</code>安装各种插件。</li></ol><ol start="4"><li>初始化, 我这里用到的是Github上的一个开源主题：<a href="https://github.com/blinkfox/hexo-theme-matery">matery</a>。参照Readme一步步安装即可。</li></ol><h4 id="写文章"><a href="#写文章" class="headerlink" title="写文章"></a>写文章</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo new post <span class="token string">"article title"</span> // 新建一篇文章<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="本地调试"><a href="#本地调试" class="headerlink" title="本地调试"></a>本地调试</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo g // 生成静态资源hexo s // 启动服务，在 http//:localhost:4000/hexo clean // 清理生成的文件<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="推送"><a href="#推送" class="headerlink" title="推送"></a>推送</h4><p>将编译好的文件推送到github即可，但不建议直接推送。参考下一章的使用CI集成。</p><h3 id="三、使用CI集成工具"><a href="#三、使用CI集成工具" class="headerlink" title="三、使用CI集成工具"></a>三、使用CI集成工具</h3><p>每次写完一篇文章,都会手动执行hexo g和hexo d去生成静态网页后在进行部署到Github page上去。而且为了保存文章的源码md文件还需要push到对应的仓库分支上,比较麻烦。那么能不能只保存原md文件push上去,其他的操作都让它自动去完成呢 ？</p><p>答案肯定是可以的,下面就来讲解一下CI具体的实现方法:</p><p>首先来介绍利用的工具:<br><code>travis</code>: 是在线托管的CI服务,用Travis来进行持续集成,不需要自己搭服务器<a href="https://travis-ci.org/">官方网站</a></p><h4 id="接入Travis过程"><a href="#接入Travis过程" class="headerlink" title="接入Travis过程:"></a>接入Travis过程:</h4><ol><li>登录Travis网站用github授权登录</li><li>登录后在个人主页选择你需要CI的仓库</li><li>点击你选择的hexo博客的仓库进行配置。点击左上角红色框的More options按钮,选择Settings打开配置页面进行配置。第一个配置项:<code>Build only if .travis.yml is present</code>代表的意思是:只有在<code>.travis.yml</code>文件中配置的分支改变了才构建。第二个配置项:<code>Build pushes</code>代表当推送完这个分支后开始构建</li></ol><p><img src="http://image.yhzhao.cn/img20210911151951.png"></p><p>到了这一步,我们已经开启了要构建的仓库,但是还有个问题就是,构建完后,我们怎么将生成的文件推送到github上呢? 我们只要想github一push,他就自动构建并push静态文件到githubpages,那么下面要解决的就是Travis CI怎么访问github了</p><ol start="4"><li>在Travis CI配置Github的<code>Access Token</code> 用来访问Github。首先我们进入github的设置页面,然后点击<code>Developer settings</code>选项进入开发者设置,然后字点击<code>Personal access tokens</code><br><img src="http://image.yhzhao.cn/img20210911152129.png"></li></ol><p>点击右上角的<code>Generate new token</code>会让你输入密码确定,然后进入一个生成token的页面。输入token的描述,选择这个token权限,然后然后点击生成就可以了,然后复制保存下来,下次在进来就看不到了。</p><ol start="5"><li><p>在 Travis 仓库配置界面setting里面 环境变量<code>Environment Variables</code>进行配置token方便在构建文件中引用: 如下图<br><img src="http://image.yhzhao.cn/img20210911152229.png"></p></li><li><p>在博客的源码文件分支下添加<code>.travis,yml</code>配置文件,决定怎么执行构建任务,下面是.travis.yml的内容:</p><pre class="line-numbers language-yml" data-language="yml"><code class="language-yml">sudo: falselanguage: node_jsnode_js:  - 10 # use nodejs v10 LTScache: npmbranches:  only:    - master # build master branch onlyscript:  - hexo generate # generate static filesdeploy:  provider: pages  skip-cleanup: true  github-token: $GH_TOKEN  keep-history: true  on:    branch: master  local-dir: public<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>将 .travis.yml 推送到 repository 中。Travis CI 应该会自动开始运行，并将生成的文件推送到同一 repository 下的 gh-pages 分支下</p></li><li><p>在 GitHub 中前往你的 repository 的设置页面，修改 GitHub Pages 的部署分支为 <code>gh-pages</code>。<br><img src="http://image.yhzhao.cn/img20210911152323.png"></p></li></ol><h3 id="四、进阶玩法"><a href="#四、进阶玩法" class="headerlink" title="四、进阶玩法"></a>四、进阶玩法</h3><h4 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h4><p>前提，你得有一个域名，有些域名需要备案后才能用。</p><ol><li>在域名解析添加记录<br>如果你用你顶点域名（如：<code>baidu.cn</code>)，就添加一条主机记录为<code>@</code>的，如果你用www子域名（如：<code>www.baidu.cn</code>, 就添加一条主机记录为<code>www</code>的, github绑定自己的域名只支持这两种，不支持其他子域名。</li></ol><ul><li>记录类型一定要为 CNAME 这种类型，只有这样你的域名才能指向你的github</li><li>记录值填 <code>yourname.github.io</code><br><img src="http://image.yhzhao.cn/img20210911153236.png"></li></ul><ol start="2"><li>在github添加自定义域名<br><img src="http://image.yhzhao.cn/img20210911153111.png"></li><li>配置hexo的_config.yml。找到url设置，添加你的域名<pre class="line-numbers language-yml" data-language="yml"><code class="language-yml">url: http://www.baidu.cn // 这里替换下root: /permalink: :year/:month/:day/:title/permalink_defaults:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li>上传CNAME文件<br>光执行上面三个步骤还是不够，每次你上传更新时，你在github设置的域名可能会丢失，所以要上传一个<code>CNAME</code>文件，让github记住你添加的域名：<br>先创建一个名为<code>CNAME</code>的文件，没有后缀，再在文件中写上你的域名（如：<code>www.baidu.cn</code>）,然后把这个文件放在<code>/source</code>目录下，上传就行了。</li></ol><h4 id="文章头设置"><a href="#文章头设置" class="headerlink" title="文章头设置"></a>文章头设置</h4><p>为了新建文章方便，建议将/scaffolds/post.md修改为如下代码：</p><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown"><span class="token front-matter-block"><span class="token punctuation">---</span><span class="token font-matter yaml language-yaml">title: {{ title }}date: {{ date }}top: falsecover: falsepassword:toc: truemathjax: truesummary:tags:categories:</span><span class="token punctuation">---</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样新建文章后不用你自己补充了，修改信息就行。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;欢迎光临&lt;a href=&quot;https://yhzhao.cn/&quot;&gt;祈梦星缘的Blog&lt;/a&gt;! 这是本站的第一篇文章，在这篇文章中会手把手的教你搭建完成 ✅ 属于自己的blog空间。快跟随我开始吧！ &lt;/p&gt;
&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Q</summary>
      
    
    
    
    <category term="踩坑记录" scheme="https://yhzhao.cn/categories/%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="建站" scheme="https://yhzhao.cn/tags/%E5%BB%BA%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>性能优化-应用分层</title>
    <link href="https://yhzhao.cn/articles/38173.html"/>
    <id>https://yhzhao.cn/articles/38173.html</id>
    <published>2021-09-10T01:10:04.000Z</published>
    <updated>2021-09-14T16:38:00.620Z</updated>
    
    
    
    
    <category term="工程实践" scheme="https://yhzhao.cn/categories/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="架构" scheme="https://yhzhao.cn/tags/%E6%9E%B6%E6%9E%84/"/>
    
    <category term="性能" scheme="https://yhzhao.cn/tags/%E6%80%A7%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>业务风控</title>
    <link href="https://yhzhao.cn/articles/43739.html"/>
    <id>https://yhzhao.cn/articles/43739.html</id>
    <published>2021-09-10T01:08:34.000Z</published>
    <updated>2021-09-14T16:38:00.620Z</updated>
    
    
    
    
    <category term="思考总结" scheme="https://yhzhao.cn/categories/%E6%80%9D%E8%80%83%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="风控" scheme="https://yhzhao.cn/tags/%E9%A3%8E%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>项目稳定性建设</title>
    <link href="https://yhzhao.cn/articles/4526.html"/>
    <id>https://yhzhao.cn/articles/4526.html</id>
    <published>2021-09-10T01:07:28.000Z</published>
    <updated>2021-09-14T16:38:00.620Z</updated>
    
    
    
    
    <category term="工程实践" scheme="https://yhzhao.cn/categories/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="分布式" scheme="https://yhzhao.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    <category term="架构" scheme="https://yhzhao.cn/tags/%E6%9E%B6%E6%9E%84/"/>
    
    <category term="稳定性" scheme="https://yhzhao.cn/tags/%E7%A8%B3%E5%AE%9A%E6%80%A7/"/>
    
  </entry>
  
  <entry>
    <title>分布式限流</title>
    <link href="https://yhzhao.cn/articles/60589.html"/>
    <id>https://yhzhao.cn/articles/60589.html</id>
    <published>2021-09-10T00:48:15.000Z</published>
    <updated>2021-09-14T16:38:00.620Z</updated>
    
    
    
    
    <category term="工程实践" scheme="https://yhzhao.cn/categories/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="分布式" scheme="https://yhzhao.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Go GC 垃圾回收</title>
    <link href="https://yhzhao.cn/articles/40153.html"/>
    <id>https://yhzhao.cn/articles/40153.html</id>
    <published>2021-09-10T00:48:03.000Z</published>
    <updated>2021-09-14T16:38:00.616Z</updated>
    
    
    
    
    <category term="计算机语言" scheme="https://yhzhao.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AF%AD%E8%A8%80/"/>
    
    
    <category term="GC" scheme="https://yhzhao.cn/tags/GC/"/>
    
    <category term="垃圾回收" scheme="https://yhzhao.cn/tags/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"/>
    
    <category term="Go" scheme="https://yhzhao.cn/tags/Go/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统原理</title>
    <link href="https://yhzhao.cn/articles/40164.html"/>
    <id>https://yhzhao.cn/articles/40164.html</id>
    <published>2021-09-10T00:47:33.000Z</published>
    <updated>2021-09-14T16:38:00.620Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、概念"><a href="#一、概念" class="headerlink" title="一、概念"></a>一、概念</h1><p><strong>分布式要解决什么问题呢</strong>？</p><ul><li><p>解决持久化数据太大，单个节点的硬盘无法存储的问题</p></li><li><p>解决运算量太大，单个节点的内存、CPU无法处理的问题</p></li></ul><p>解决这些问题，有两种思路：**scale up(纵向扩展) ，scale out(横向扩展)**。前者就是提升单个节点的能力，更大的磁盘，更快的CPU，定制的软硬件，然而这意味着更高的价格，而且再怎么scale up 也是有上限的。后者就是把存储、计算任务分担到普通的机器上，通过动态增加节点来应对数据量的增长，但缺点是多个节点的管理、任务的调度比较复杂，这也是分布式系统研究和解决的问题。</p><p>只有当数据量达到单机无法存储、计算的情况下才考虑分布式，不然都是自找麻烦。状态的维护比计算要难很多。分布式系统要做的任务就是把多台机器有机的组合、连接起来，让其协同完成一件任务，可以是计算任务，也可以是存储任务</p><ul><li>分布式存储</li><li>分布式计算</li></ul><blockquote><p><font color="red">Q: 为什么大多都会将分布式系统分为存储和计算？或者除了实际完成的功能区别，还有哪些不同</font></p><p>分布式系统是将在单机完成的存储和计算，分发到多个几点进行处理</p></blockquote><p>由于分布式系统多节点、通过网络通信的拓扑结构，因此会引入很多单机系统没有的问题，为了解决这些问题又会引入更多的机制、协议，带来更多的问题。。。。</p><h2 id="1-模型"><a href="#1-模型" class="headerlink" title="1. 模型"></a>1. 模型</h2><h4 id="1-1-分片（partition）。"><a href="#1-1-分片（partition）。" class="headerlink" title="1.1 分片（partition）。"></a>1.1 <strong>分片（partition）</strong>。</h4><p>对于计算，那么就是对计算任务进行切换，每个节点算一些，最终汇总就行了，MapReduce的思想；对于存储，每个节点存一部分数据就行了。当数据规模变大的时候，Partition是唯一的选择，同时也会带来一些好处：</p><ul><li>提升性能和并发，操作被分发到不同的分片，相互独立</li><li>提升系统的可用性，即使部分分片不能用，其他分片不会受到影响</li></ul><p><img src="http://image.yhzhao.cn/img20210911204452.png"></p><p>理想的情况下，有分片就能解决之前提到的资源不足的问题。</p><h4 id="1-2-异常"><a href="#1-2-异常" class="headerlink" title="1.2 异常"></a>1.2 异常</h4><p>​    任何在设计阶段考虑到的异常情况一定会在系统实际运行中发生，但在系统实际运行遇到的异常却很有可能在设计时未能考虑，所以，除非需求指标允许，在系统设计时不能放过任何异常情况。</p><h5 id="机器宕机"><a href="#机器宕机" class="headerlink" title="机器宕机"></a>机器宕机</h5><h5 id="网络异常"><a href="#网络异常" class="headerlink" title="网络异常"></a>网络异常</h5><blockquote><p>消息丢失，两片节点之间彼此完全无法通信，即出现了“网络分化”；</p><p>消息乱序，有一定的概率不是按照发送时的顺序依次到达目的节点，考虑使用序列号等机制处理网络消息的乱序问题，使得无效的、过期的网络消息不影响系统的正确性；</p><p>数据错误，数据传输过程中发生了错误。</p></blockquote><h5 id="三态"><a href="#三态" class="headerlink" title="三态"></a><strong>三态</strong></h5><blockquote><p>如果某个节点向另一个节点发起RPC调用，即某个节点A 向另一个节点B 发送一个消息，节点B 根据收到的消息内容完成某些操作，并将操作的结果通过另一个消息返回给节点A，那么这个RPC 执行的结果有三种状态：成功、失败、超时（未知）</p></blockquote><h5 id="存储数据丢失"><a href="#存储数据丢失" class="headerlink" title="存储数据丢失"></a><strong>存储数据丢失</strong></h5><blockquote><p> 对于有状态节点来说，数据丢失意味着状态丢失，通常只能从其他节点读取、恢复存储或状态</p></blockquote><h4 id="1-3-冗余（Replication）"><a href="#1-3-冗余（Replication）" class="headerlink" title="1.3 冗余（Replication）"></a>1.3 <strong>冗余（Replication）</strong></h4><p>分布式系统中有大量的节点，且通过网络通信。单个节点的故障（进程crash、断电、磁盘损坏）是个小概率事件，但整个系统的故障率会随节点的增加而指数级增加，网络通信也可能出现断网、高延迟的情况。在这种一定会出现的“异常”情况下，分布式系统还是需要继续稳定的对外提供服务，即需要较强的容错性。</p><blockquote><p><font color="red">Q: TCP 协议是可靠的、面向连接的传输服务，但在分布式系统的协议设计中不能认为所有网络通信都基于TCP 协议则通信就是可靠的？</font></p><ul><li>TCP协议只能保证同一个TCP 链接内的网络消息不乱序，TCP 链接之间的网络消息顺序则无法保证。</li><li>当应用程序通过调用系统函数send发送一个TCP数据流时，函数返回成功，但是这仅仅意味着该消息被本机的网络协议栈所接收。即使数据被对端的网络协议栈所接收，并返回确认包，也仅仅以为这数据到达了对端的协议栈。不能假设对端上层应用已经接收并正确处理。所以消息是可能丢失的。</li><li>拜占庭将军</li></ul></blockquote><p>解决方式是由多个节点负责同一个任务，最为常见的就是分布式存储中，多个节点负责存储同一份数据，以此增强可用性与可靠性。同时，冗余也会带来性能的提升，比如数据的本地化可以减少用户的等待时间。</p><blockquote><p><font color="red">Q: 有哪些具体的关于冗余存储带来的可靠性和性能提升的案例？</font></p><p><font color="red">Q: 无论是存储还是计算，需要先定位数据再进行操作，相关的解决方案？</font></p></blockquote><p><img src="http://image.yhzhao.cn/img20210911204314.png"></p><p>Partitioned是把数据分成相互独立的数据集，这样可以避免数据的增长带来的影响, 同时可以解决单点故障，提高了系统的可用性。Replication意味着数据被完整的拷贝到了多个节点上。比如三备份存储，提高系统的读IO；Replication多数据中心备份，降低网络延迟。</p><blockquote><p><font color="red">Q: 对于Partitioned和Replication来说：一定意义上分片能够解决计算过程中单点故障，冗余解决了存储过程中的单点故障，提高了可用性。但冗余带来的读写性能提高是额外收益吗？两者的关系如何？</font></p><p>类比磁盘阵列（RAID）的工作原理</p><p><font color="red">Q: 涉及到的<a href="https://blog.csdn.net/u011026968/article/details/52295666">纠删码</a>问题</font></p></blockquote><h4 id="1-4-一致性（consistency）"><a href="#1-4-一致性（consistency）" class="headerlink" title="1.4 一致性（consistency）"></a>1.4 <strong>一致性（</strong>consistency<strong>）</strong></h4><p>Partition和Replication是解决分布式系统问题的一记组合拳，很多具体的问题都在用这个思路去解决。</p><p>但这并不是银弹，往往是为了解决一个问题，会引入更多的问题。为了可用性与可靠性保证，引用了冗余。有了冗余，各个<strong>副本间的一致性问题</strong>就变得很头疼，一致性在系统的角度和用户的角度又有不同的等级划分。</p><ul><li><p>如果要保证强一致性，那么会影响可用性与性能，在一些应用（比如电商、搜索）是难以接受的。</p></li><li><p>如果是最终一致性，那么就需要处理数据冲突的情况。</p><blockquote><p> <a href="https://zhuanlan.zhihu.com/p/23278877">CAP</a>、<a href="https://nicky-chen.github.io/2018/04/25/cap-base-flp/">FLP</a>这些理论告诉我们，在分布式系统中，没有最佳的选择，都是需要权衡，做出最合适的选择。牺牲一致性而换取高可用性，是比较常用的方式</p></blockquote></li></ul><h5 id="强一致性（Strong-）"><a href="#强一致性（Strong-）" class="headerlink" title="强一致性（Strong ）"></a><strong>强一致性（Strong ）</strong></h5><p>当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值。根据 CAP 理论，这种实现需要牺牲可用性。</p><ul><li>任何一次读都能读到某个数据的最近一次写的数据。</li><li>系统中的所有进程，看到的操作顺序，都和全局时钟下的顺序一致。</li></ul><blockquote><p><font color="red">Q: 分布式系统的一致性和Mysql的事务一致性是怎样的关系</font></p><p>两者不是一个概念，mysql的两阶段提交指的是一个记录的完成过程。</p><p><img src="http://image.yhzhao.cn/img20210415203516.png"></p><img src="http://image.yhzhao.cn/img20210415203602.png" style="zoom:33%;"><ul><li><a href="https://www.zhihu.com/question/275845393/answer/397349131">知乎回答</a></li></ul><p><font color="red">Q: 强一致性的实现方式，应用场景</font></p><p><font color="red">Q: 事务一致性和数据一致性</font></p></blockquote><h6 id="两阶段提交"><a href="#两阶段提交" class="headerlink" title="两阶段提交"></a><em>两阶段提交</em></h6><p><img src="http://image.yhzhao.cn/img20210414215104.png"></p><p><img src="http://image.yhzhao.cn/img20210414215650.png"></p><p><strong>同步阻塞问题</strong>。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。</p><p><strong>单点故障</strong>。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。</p><p><strong>数据不一致</strong>。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。</p><p><strong>二阶段无法解决的问题</strong>当协调者出错，同时参与者也出错时，两阶段无法保证事务执行的完整性。考虑协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</p><h6 id="三阶段提交"><a href="#三阶段提交" class="headerlink" title="三阶段提交"></a><em>三阶段提交</em></h6><ul><li>引入超时机制。同时在协调者和参与者中都引入超时机制。（两阶段提交只有协调者存在超时）</li><li>在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的</li></ul><p>三阶段提交有<code>CanCommit</code>、<code>PreCommit</code>、<code>DoCommit</code>三个阶段。3PC主要解决的<strong>单点故障问题，并减少阻塞</strong>，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况</p><pre class="line-numbers language-none"><code class="language-none">在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（*一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了*）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="http://image.yhzhao.cn/img20210911204349.png"></p><blockquote><p> <font color="red">Q: 为什么要变成3阶段，如果还是两阶段提交，在参与者那里引入超时是否也能达到相同效果</font></p><p>其实是为了第三步的可以默认执行，如果收到了pre的消息，说明大家大概率会提交。可以理解为将两阶段提交的第一步做了拆分。</p></blockquote><p><em>there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos</em> </p><p>​                                                                                                                                                                    — Mike Burrows</p><h5 id="顺序一致性（Sequential）"><a href="#顺序一致性（Sequential）" class="headerlink" title="顺序一致性（Sequential）"></a><strong>顺序一致性（Sequential）</strong></h5><ul><li>任何一次读都能读到某个数据的最近一次写的数据。</li><li>系统的所有进程的顺序一致，而且是合理的。即不需要和全局时钟下的顺序一致，错的话一起错，对的话一起对。</li></ul><p><img src="http://image.yhzhao.cn/img20210911203755.png"></p><blockquote><p><font color="red">Q: 哪些场景下需要顺序一致性的保证？</font></p><p>内存模型、Zookeeper的Zab协议</p></blockquote><h5 id="最终一致性（Eventually）"><a href="#最终一致性（Eventually）" class="headerlink" title="最终一致性（Eventually）"></a><strong>最终一致性（Eventually）</strong></h5><p>当用户从异步从库读取时，果此异步从库落后，他可能会看到过时的信息。这种不一致只是一个暂时的状态，如果等待一段时间，从库最终会赶上并与主库保持一致。这称为<strong>最终一致性。</strong></p><h6 id="读写一致性"><a href="#读写一致性" class="headerlink" title="读写一致性"></a><em>读写一致性</em></h6><p>​    回复某个帖子然后想马上查看，但刚提交的回复可能尚未到达从库，看起来好像是刚提交的数据丢失了。在这种情况下，我们需要<strong>读写一致性</strong>。它可以保证，如果用户刷新页面，他们总会看到自己刚提交的任何更新。它不会对其他用户的写入做出承诺，其他用户的更新可能稍等才会看到，但它保证用户自己提交的数据能马上被自己看到。– <em>读己之所写一致性</em></p><blockquote><p>如何实现读写一致性？</p><ol><li>最简单的方案，<strong>对于某些特定的内容，都从主库读。</strong>个人主页信息只能由用户本人编辑，而不能由其他人编辑。因此，永远从主库读取用户自己的个人主页，从从库读取其他用户的个人主页。</li><li>如果应用中的大部分内容都可能被用户编辑的情况下可以使用其他标准来决定是否从主库读取，例如可以<strong>记录每个用户最后一次写入主库的时间</strong>，一分钟内都从主库读，同时<strong>监控从库的最后同步时间</strong>，任何超过一分钟没有更新的从库不响应查询。</li><li>还有一种更好的方法是，客户端可以<strong>在本地记住最近一次写入的时间戳</strong>，发起请求时带着此时间戳。从库提供任何查询服务前，需确保<strong>该时间戳前的变更都已经同步到了本从库中</strong>。如果当前从库不够新，则可以从另一个从库读，或者等待从库追赶上来。</li></ol></blockquote><h6 id="因果一致性"><a href="#因果一致性" class="headerlink" title="因果一致性"></a><em>因果一致性</em></h6><p>​    需要保证因果一致性的场景往往发生在<strong>分区</strong>（也称为<strong>分片</strong>）的分布式数据库中。分区后，每个节点并不包含全部数据。不同的节点独立运行，因此不存在<strong>全局写入顺序。</strong>如果用户A发布一篇文章，用户B提交一个回复。问题写入了节点A，回复写入了节点B。因为同步延迟，发起查询的用户可能会先看到回复，再看到文章。为了防止这种异常，需要<strong>因果一致性</strong>的保证。 即如果一系列写入按某个逻辑顺序发生，那么任何人读取这些写入时，会看见它们以正确的逻辑顺序出现。</p><blockquote><p> 这是一个实际却很难解决的问题。一种方案是应用保证将问题和对应的回答写入相同的分区。但并不是所有的数据都能如此轻易地判断因果依赖关系。</p><p>Todo 向量时钟</p></blockquote><h6 id="单调读"><a href="#单调读" class="headerlink" title="单调读"></a><em>单调读</em></h6><p>​    用户从某从库查询到了一条记录，再次刷新后发现此记录不见了。如果用户从不同从库进行多次读取，就可能发生这种情况。<strong>单调读</strong>可以保证这种异常不会发生。单调读意味着如果一个用户进行多次读取时，如果先前读取到较新的数据，后续读取不会得到更旧的数据。<strong>单调读</strong>比强一致性更弱，比最终一致性更强。</p><blockquote><p>实现单调读取的一种方式是<strong>确保每个用户总是从同一个节点进行读取</strong>（不同的用户可以从不同的节点读取），比如可以基于用户ID的哈希值来选择节点，而不是随机选择节点。</p></blockquote><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>对于分布式系统的一致性，前述举例都是在分布式存储的应用中。在<strong>分布式计算</strong>中，以上概念有哪些应用和解决方式？</p><h2 id="2-分布式系统特性总结"><a href="#2-分布式系统特性总结" class="headerlink" title="2. 分布式系统特性总结"></a>2. 分布式系统特性总结</h2><h4 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h4><p>​    分布式系统的根本目标就是为了处理单个计算机无法处理的任务，<strong>当任务增加的时候，分布式系统的处理能力需要随之增加</strong>。要比较方便的通过增加机器来应对数据量的增长，同时，当任务规模缩减的时候，可以撤掉一些多余的机器，达到动态伸缩的效果。好的分布式系统总在追求“线性扩展性”，也就是使得系统的某一指标可以随着集群中的机器数量<strong>线性增长</strong>。</p><p><font color="red">理想目标：追求线性扩展</font></p><p>为了满足这一特性，涉及到的问题：在动态增加节点的时候，需要进行任务（可能是计算，可能是数据存储）的迁移，以达到动态均衡。如何对任务进行拆分，将任务的子集分配到每一个节点。 </p><p><strong>待解决的问题： <em>分布式系统分片方式</em></strong></p><h4 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h4><p>​    系统的可用性(availability)指系统在面对各种异常时可以正确提供服务的能力。系统的可用性可以用系统停服务的时间与正常服务的时间的比例来衡量，也可以用某功能的失败次数与成功次数的比例来衡量。可用性是分布式的重要指标，衡量了系统的鲁棒性，是系统容错能力的体现。</p><p>​    分布式系统由大量异构的节点和网络组成，节点可能会crash、断电、磁盘损坏，网络可能丢包、延迟、网络分割。系统的规模放大了出故障的概率，因此分布式系统中，故障是常态。那么分布式系统的其中一个设计目标就是容错，在部分故障的情况下仍然对外提供服务</p><p><font color="red">追求目标就是7 * 24，即永远在线</font></p><p><img src="http://image.yhzhao.cn/img20210911204412.png"></p><p><strong>冗余(副本机制)是提高可用性、可靠性的法宝</strong>。</p><p>冗余就是说多个节点负责相同的任务，在需要状态维护的场景，比如分布式存储中使用非常广泛。在分布式计算，如MapReduce中，当一个worker运行异常缓慢时，master会将这个worker上的任务重新调度到其它worker，以提高系统的吞吐，这也算一种冗余。但存储的冗余相比计算而言要复杂许多，因此主要考虑存储的冗余。</p><p><em>怎么保证并发情况下各个副本数据的一致性，是否有一个节点有决定更新的顺序，这就是中心化、去中心化副本协议的区别</em></p><p><strong>待解决的问题： <em>副本控制协议</em></strong></p><h4 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h4><p>​    分布式系统为了提高可用性，总是不可避免的使用副本的机制，从而引发副本一致性的问题。越是强的一致的性模型，对于用户使用来说使用起来越简单</p><p>​    CAP理论就是说分布式数据存储，最多只能同时满足一致性（C，Consistency）、可用性（A， Availability）、分区容错性（P，Partition Tolerance）中的两者。但一致性和可用性都是一个度的问题，是0到1，而不是只有0和1两个极端</p><p>​    对于分布式数据系统：</p><pre class="line-numbers language-none"><code class="language-none">N — 数据复制的份数W — 更新数据时需要保证写完成的节点数R — 读取数据的时候需要读取的节点数    1.如果W+R&gt;N：则是强一致性，写的节点和读的节点重叠。例如对于典型的一主一备同步复制的关系型数据库。N=2,W=2,R=1，则不管读的是主库还是备库的数据，都是一致的。    2.如果W+R&lt;=N：则是弱一致性。例如对于一主一备异步复制的关系型数据库，N=2,W=1,R=1，则如果读的是备库，就可能无法读取主库已经更新过的数据，所以是弱一致性。    对于分布式系统，为了保证高可用性，一般设置N&gt;=3。不同的N,W,R组合，是在可用性和一致性之间取一个平衡，以适应不同的应用场景。如果N=W,R=1，任何一个写节点失效，都会导致写失败，因此可用性会降低，但是由于数据分布的N个节点是同步写入的，因此可以保证强一致性。如果N=R,W=1，只需要一个节点写入成功即可，写性能和可用性都比较高。但是读取其他节点的进程可能不能获取更新后的数据，因此是弱一致性。这种情况下，如果W&lt;(N+1)/2，并且写入的节点不重叠的话，则会存在写冲突 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="高性能"><a href="#高性能" class="headerlink" title="高性能"></a>高性能</h4><ul><li><p>系统的吞吐能力，指系统在某一时间可以处理的数据总量，通常可以用系统每秒处理的总的数据量来衡量；</p></li><li><p>系统的响应延迟，指系统完成某一功能需要使用的时间；</p></li><li><p>系统的并发能力，指系统可以同时完成某一功能的能力，通常也用QPS(query per second)来衡量。</p></li></ul><p>三个性能指标往往会相互制约，追求高吞吐的系统，往往很难做到低延迟；系统平均响应时间较长时，也很难提高QPS。</p><h2 id="3-原理"><a href="#3-原理" class="headerlink" title="3. 原理"></a>3. 原理</h2><h3 id="3-1-数据分布方式"><a href="#3-1-数据分布方式" class="headerlink" title="3.1 数据分布方式"></a>3.1 数据分布方式</h3><p>​    将一个单机问题使用分布式解决，首先要解决的就是如何将问题拆解为可以使用多机分布式解决，使得分布式系统中的每台机器负责原问题的一个子集。由于无论是计算还是存储，其问题处理的对象都是数据，数据不存在于一台机器或进程中，所以如何拆解分布式系统的输入数据成为分布式系统的基本问题。</p><h4 id="哈希取模"><a href="#哈希取模" class="headerlink" title="哈希取模"></a>哈希取模</h4><p>​    哈希方式是最常见的数据分布方式，实现方式是通过可以描述记录的业务的id或key(比如用户 id)，通过Hash函数的计算求余。余数作为处理该数据的服务器索引编号处理。<img src="http://image.yhzhao.cn/img20210911203831.png"></p><p><em>优点</em>：只需要通过计算就可以映射出数据和处理节点的关系，不需要存储映射</p><p><em>缺点</em>：如果id分布不均匀可能出现计算、存储倾斜的问题，在某个节点上分布过重。并且当处理节点宕机时，这种”硬哈希“的方式会直接导致部分数据异常。</p><p><strong>扩容问题</strong>：如果是”无状态“型的节点，影响比较小，但遇到”有状态“的存储节点需要扩容时，几乎所有的数据需要被迁移并重新分布。工程上的一种解决方式是, 使得集群规模成倍(按2的幂的机器数)扩展，按照数据重新计算哈希。原本一台机器上的数据只需迁移一半到另一台对应的机器上即可完成扩展。<img src="http://image.yhzhao.cn/img20210911204122.png"></p><p>另外一种解决方式是不再简单的将哈希值与机器做除法取模映射，而是将<strong>对应关系作为元数据由专门的元数据服务器管理</strong>。同时，哈希值取模个数往往大于机器个数，这样同一台机器上需要负责多个哈希取模的余数。但需要以较复杂的机制维护大量的元数据</p><h4 id="一致性哈希"><a href="#一致性哈希" class="headerlink" title="一致性哈希"></a>一致性哈希</h4><p>​    使用一个哈希函数计算数据或数据特征的哈希值，令该哈希函数的输出值域为一个封闭的环，最大值+1=最小值。将节点随机分布到这个环上，每个节点负责处理从自己开始顺时针至下一个节点的全部哈希值域上的数据.<img src="http://image.yhzhao.cn/img20210415004119.png" style="zoom:50%;"><img src="http://image.yhzhao.cn/img20210415004344.png" style="zoom:50%;"></p><p><em>优点</em>：任意动态添加、删除节点，每次添加、删除一个节点仅影响一致性哈希环上相邻的节点。 为了尽可能均匀的分布节点和数据，一种常见的改进算法是引入虚节点的概念，系统会创建许多虚拟节点，个数远大于当前节点的个数，均匀分布到一致性哈希值域环上。读写数据时，首先通过数据的哈希值在环上找到对应的虚节点，然后查找到对应的real节点。这样在扩容和容错时，大量读写的压力会再次被其他部分节点分摊，主要解决了压力集中的问题。</p><blockquote><p>一旦某个节点不可用，该节点将使得多个虚节点不可用，从而使得多个相邻的真实节点负担失效节点的压力。同理，一旦加入一个新节点，可以分配多个虚节点，从而使得新节点可以 负担多个原有节点的压力，从全局看，较容易实现扩容时的负载均衡</p><p>使用一致性哈希的方式需要将节点在一致性哈希环上的位置作为元信息加以管理，这点比直接使用哈希分布数据的方式要复杂。然而，节点的位置信息只于集群中的机器规模相关，且元信息量要小很多</p></blockquote><ul><li>为什么元信息量会小很多</li></ul><h4 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h4><p>​    将数据按特征值的值域范围划分为不同的区间，使得集群中每台（组）服务器处理不同区间的数据。为了数据迁移等负载均衡操作的方便，往往利用动态划分区间的技术，使得每个区间中服务的数据量尽量的一样多。当某个区间的数据量较大时，通过将区间“分裂”的方式拆分为两个区间，使得每个数据区间中的数据量都尽量维持在一个较为固定的阈值之下。</p><p>​    缺点是由于数据分布信息不能通过计算获取，需要引入一个模块存储这些映射信息。这就增加了模块依赖，可能会有性能和可用性的额外代价。且一般元信息的数据量比较大。</p><blockquote><p>哈希分布数据的方式使得系统中的数据类似一个哈希表。按范围分数据的方式则使得从全局看 数据类似一个 B 树。每个具体的服务器都是 B 树的叶子节点，元数据服务器是 B 树的中间节点。</p><p>使用范围分布数据的方式的最大优点就是可以灵活的根据数据量的具体情况拆分原有数据区间， 拆分后的数据区间可以迁移到其他机器，一旦需要集群完成负载均衡时，与哈希方式相比非常灵活。 另外，当集群需要扩容时，可以随意添加机器，而不限为倍增的方式，只需将原机器上的部分数据 分区迁移到新加入的机器上就可以完成集群扩容。</p><p>按范围分布数据方式的缺点是需要维护较为复杂的元信息。随着集群规模的增长，元数据服务 器较为容易成为瓶颈，从而需要较为负责的多元数据服务器机制解决这个问题。</p></blockquote><img src="http://image.yhzhao.cn/img20210415005542.png" style="zoom:33%;"><blockquote><p>HBase</p></blockquote><h4 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h4><p>​        许多文件系统经常采用类似设计，与具体的数据特征无关，而是将数据视为一个顺序增长的文件，并将这个文件按照某一较为固定的大小划分为若干数据块，将数据块均匀的分布在各个节点，这种做法也需要外部节点来存储映射关系。由于与具体的数据内容无关，按数据量分布数据的方式一般没有数据倾斜的问题，数据总是被均匀切分并分布到集群中。当集群需要重新负载均衡时，只需通过迁移数据块即可完成。集群扩容也没有太大的限制，只需将部分数据库迁移到新加入的机器上即可以完成扩容。</p><p><img src="http://image.yhzhao.cn/img20210911203921.png"></p><p>HDFS</p><h3 id="3-2-数据副本"><a href="#3-2-数据副本" class="headerlink" title="3.2 数据副本"></a>3.2 数据副本</h3><p>​    分布式系统容错、提高可用性的基本手段就是使用副本。对于数据副本的分布方式主要影响系统的可扩展性。一种基本的数据副本策略是以机器为单位，若干机器互为副本，副本机器之间的数据完全相同。这种策略适用于上述各种数据分布方式。其优点是非常简单，其缺点是恢复数据的效率不高、可扩展性也不高。</p><blockquote><p>假设有 3 个副 本机器，某时刻其中某台机器磁盘损坏，丢失了全部数据，此时使用新的机器替代故障机器，为了 是的新机器也可以提供服务，需要从正常的两台机器上拷贝数据。此种全盘拷贝数据一般都较为消 耗资源，为了不影响服务质量，实践中往往采用两种方式:</p><p>一、将一台可用的副本机器下线，专门 作为数据源拷贝数据，这样做的缺点是造成实际正常副本数只有 1 个，对数据安全性造成巨大隐患， 且如果服务由于分布式协议设计或压力的要求必须 2 个副本才能正常工作，则该做法完全不可行。</p><p>二、以较低的资源使用限速的方法从两个正常副本上拷贝数据，此方法不停服务，但可以选择服务 压力较小的时段进行。该方法的缺点是速度较慢，如果需要恢复的数据量巨大(例如数 T)，限速 较小(例如 10MB/s)，往往需要数天才能够完成恢复。</p><p>mysql实现</p></blockquote><p>​    理想的情况是，若集群有 N 台机器，宕机一台后，该台机器的压力可以均匀分散到剩 下的 N-1 台机器上，每台机器的压力仅仅增加 1/N-1。更合适的做法不是以机器作为副本单位，而是<strong>将数据拆为较合理的数据段，以数据段为单位作为副本</strong>。实践中，常常使得每个数据段的大小尽量相等且控制在一定的大小以内。数据段有很多不同的称谓，segment，fragment，chunk，partition 等等。数据段的选择与数据分布方式直接相关。对于哈希分数据的方式，每个哈希分桶后的余数可以作为一个数据段，为了控制数据段的大小，常常使得分桶个数大于集群规模。一旦将数据分为数据段，则可以以数据段为单位管理副本，从而副本与机器不再硬相关，每台机器都可以负责一定数据段的副本。</p><img src="http://image.yhzhao.cn/img20210415010244.png" style="zoom:50%;"><p><em>优点：</em></p><p>​    数据丢失后的恢复效率将非常高</p><blockquote><p>一旦某台机器的数据丢失，其上数据段的副本将分布在整个集群的所有机器中，而不是仅在几个副本机器中，从而可以从整个集群同时拷贝恢复数据，而集群中每台数据源机器都可以以非常低的资源做拷贝。作为恢复数据源的机器即使都限速1MB/s，若有100 台机器参与恢复，恢复速度也能达到100MB/s</p></blockquote><p>​    容错性能以及集群扩展较好</p><blockquote><p>如果出现机器宕机，由于宕机机器上的副本分散于整个集群，其压力也自然分散到整个集群。集群扩展时，设集群规模 为N 台机器，当加入一台新的机器时，只需从各台机器上迁移1/N – 1/N+1 比例的数据段到新机器即实现了新的负载均衡</p></blockquote><p>工程中，完全按照数据段建立副本会引起需要管理的元数据的开销增大，副本维护的难度也相应增大。一种折中的做法是将某些数据段组成一个数据段分组，按数据段分组为粒度进行副本管理。这样做可以将<strong>副本粒度控制</strong>在一个较为合适的范围内。</p><blockquote><p>Kafka的实现</p></blockquote><h4 id="方式选择"><a href="#方式选择" class="headerlink" title="方式选择"></a>方式选择</h4><p>组合拳：数据分段弥补哈希方式带来的数据倾斜问题</p><blockquote><p><font color="red">Q：一些经典的分布式系统的实现方式</font></p></blockquote><img src="http://image.yhzhao.cn/img20210415125234.png" style="zoom:35%;"><h3 id="3-3副本控制协议"><a href="#3-3副本控制协议" class="headerlink" title="3.3副本控制协议"></a>3.3副本控制协议</h3><p>​    副本控制协议指按特定的协议流程控制副本数据的读写行为，使得副本满足一定的可用性和一致性要求的分布式协议。但要设计一种满足强一致性，且在出现任何网络异常时都可用的副本协议是不可能的。</p><p>“中心化(centralized)副本控制协议”和“去中心化(decentralized) 副本控制协议”</p><blockquote><p> <font color="red">Q：对于分布式计算来说副本控制协议有哪些作用？</font></p></blockquote><h4 id="3-3-1-中心化副本控制协议"><a href="#3-3-1-中心化副本控制协议" class="headerlink" title="3.3.1 中心化副本控制协议"></a>3.3.1 中心化副本控制协议</h4><p>​    由一个中心节点协调副本数据的更新、维护副本之间的一致性。比如Primary-secondary协议。</p><img src="http://image.yhzhao.cn/img20210415172502.png" style="zoom:60%;"><p>优点：</p><ul><li><p>协议相对较为简单，所有的副本相关的控制交由中心节点完成。从而使得一个分布式并发控制问题，简化为一个单机并发控制问题</p><blockquote><p>多个节点同时需要修改副本数据时，需要解决“写写”、“读写”等并发冲突。单机系统上常用加锁等方式进行并发控制。对于分布式并发控制，加锁也是一个常用的方法，但如果没有中心节点统一进行锁管理，就需要完全分布式化的锁系统，会使得协议非常复杂</p></blockquote></li></ul><p>缺点</p><ul><li>系统的可用性依赖于中心化节点，当中心节点异常或与中心节点通信中断时，系统将失去某些服务。常表现为无法更新</li></ul><h5 id="1-Primary-secondary协议"><a href="#1-Primary-secondary协议" class="headerlink" title="1. Primary-secondary协议"></a>1. Primary-secondary协议</h5><p>Primary-secondary协议中副本被分为两大类，其中仅有一个副本作为primary副本，其他为secondary副本。维护 primary 副本的节点作为中心节点，中心节点负 责维护数据的更新、并发控制、协调副本的一致性。其协议需要解决四大问题：数据更新流程、数据读取方式、Primary副本的确定和切换、数据同步</p><blockquote><p>两阶段提交的一个实现？</p></blockquote><p><strong>数据更新流程</strong></p><ol><li>数据更新都由primary 节点协调完成。</li><li>外部节点将更新操作发给primary 节点</li><li>primary 节点进行并发控制即确定并发更新操作的先后顺序</li><li>primary 节点将更新操作发送给secondary 节点</li><li>primary 根据secondary 节点的完成情况决定更新是否成功并将结果返回外部节点</li></ol><img src="http://image.yhzhao.cn/img20210415173500.png" style="zoom:50%;"><blockquote><p>如果由primary 直接同时发送给其他N 个副本发送数据，则每个 <strong>secondary 的更新吞吐受限于primary 总的出口网络带宽</strong>，最大为primary 网络出口带宽的1/N。为了解决这个问题，有些系统使用接力的方式同步数据，即primary 将更新发送给第一 个secondary 副本，第一个secondary 副本发送给第二secondary 副本，依次类推。</p></blockquote><p>异常：如果在第四步出现了网络异常如何处理？Quorum机制</p><p>应该要和系统的一致性要求相关，如果是最终一致性的实现。可以允许失败，后续会慢慢更新到相应的节点。第五步的操作强依赖与第四步的要求。</p><p><strong>数据读取</strong></p><p>数据读取方式与一致性高度相关。</p><ul><li><p>最终一致性，则读取任何副本都可以满足需求。</p></li><li><p>单调读一致性，则可以为副本设置版本号，每次更新后递增版本号，用户读取副本时验证版本号，从而保证用户读到的数据在会话范围内单调递增</p></li><li><p>强一致性，比较困难，有下面几种实现思路</p><ul><li><p>只读primary 副本。secondary 副本将不提供读服务。</p><blockquote><p>实践中，如果副本不与机器绑定，而是按照数据段为单位维护副本，仅有primary 副本提供读服务在很多场景下并不会造出机器资源浪费，假设primary 也是随机的确定的，那么每台机器上都有一些数据的primary 副本，也有另一些数据段的secondary 副本。从而某台服务器实际都提供读写服务</p><p><em>元信息需要维护</em></p></blockquote></li><li><p>由primary 控制节点secondary 节点的可用性。当primary 更新某个secondary 副本不成功时，primary 将该secondary 副本标记为不可用，从而用户不再读取该不可用的副本。不可用的 secondary 副本可以继续尝试与primary 同步数据，当与primary 完成数据同步后，primary 可以副本标记为可用。</p><blockquote><p>这种方式使得所有的可用的副本，无论是primary 还是secondary 都是可读的，且在一个确定的时间内，某secondary 副本要么更新到与primary 一致的最新状态，要么被标记为不可用，从而符合较高的一致性要求。这种方式依赖于一个中心元数据管理系统，用于记录哪些副本可用，哪些副本不可用。该方式通过降低系统的可用性来提高系统的一致性。</p></blockquote></li></ul></li></ul><p><strong>primary 副本的确定与切换</strong></p><p>​    在primary-secondary 类型的分布式系统中，哪个副本是primary 这一信息都属于元信息，由专门的元数据服务器维护。执行更新操作时，首先查询元数据服务器获取副本的primary 信息，从而进一步执行数据更新流程。在原primary 副本所在机器出现宕机等异常时，可能会导致服务停服。</p><ul><li><p>如何确定节点的状态以发现原 primary 节点异常，异常探测。lease</p></li><li><p>换 primary后，不能影响副本的一致性。Quorum</p><blockquote><p>切换的新 primary的副本数据必须与原primary的副本一致。然而在原 primary 已经发送宕机等异常时，如何确定一个secondary 副本使得该副本上的数据与原primary 一致又成为新的问题。就变成了如何确定读取最新数据的问题。</p></blockquote></li></ul><p><strong>数据同步</strong></p><p>不一致的secondary 副本需要与primary 进行同步。</p><p>通常不一致的形式有三种：</p><ul><li><p>由于网络分化等异常，secondary 上的数据落后于primary 上的数据。</p><blockquote><p>回放primary 上的redo 日志</p></blockquote></li><li><p>在某些协议下，secondary 上的数据有可能是脏数据，需要被丢弃。</p><blockquote><p>直接丢弃有脏数据的副本，这样相当于副本没有数据。</p><p>设计一些基于 undo 日志的方式从而可以删除脏数据。</p><p><em>如何做到的？日志机制</em></p></blockquote></li><li><p>secondary 是一个新增加的副本，完全没有数据，需要从其他副本上拷贝数据。</p><blockquote><p>直接拷贝 primary 副本的数据，这种方法往往比回放日志追更新进度的方法快很多。但拷贝数据时 primary 副本需要能够继续提供更新服务，这就要求 primary 副本支持快照(snapshot)功能。即对某一刻的副本数据形成快照，然后拷贝快照，拷贝完成后使用回放日志的方式追回快照形成后的更新操作。</p></blockquote></li></ul><h5 id="2-Lease-机制"><a href="#2-Lease-机制" class="headerlink" title="2. Lease 机制"></a>2. Lease 机制</h5><blockquote><p>1989年斯坦福大学的Cary G. Gray和David R. Cheriton</p><p>如何确定异常节点的状态</p></blockquote><p>租约机制，是一种在分布式系统常用的协议，是维护分布式系统数据一致性的一种常用工具。 Lease机制有以下几个特点： </p><ol><li>Lease是颁发者对一段时间内数据一致性的承诺； </li><li>颁发者发出Lease后，不管是否被接收，只要Lease不过期，颁发者都会按照协议遵守承诺；</li><li> Lease的持有者只能在Lease的有效期内使用承诺，一旦Lease超时，持有者需要放弃执行，重新申请Lease。</li></ol><p>当有更改请求时，服务器修改了数据，但是缓存却还没来得及修改，就带来了数据一致性的问题。同时系统要能最大可能的处理节点宕机、网络中断等 异常，最大程度的提高系统的可用性。</p><p><strong>基本原理：</strong></p><ul><li><p>中心服务器在向各节点发送数据时同时向节点颁发一个 lease。每个 lease 具有一个有效期，和信用卡上的有效期类似，lease 上的有效期通常是一个明确的时间点，一旦真实时间超过这个时间点，则 lease 过期失效。</p></li><li><p>lease 的有效期与节点收到 lease 的时间无关，节点可能收到 lease 时该 lease 就已经过期失效。 </p><blockquote><p>如何保证时钟同步?</p><p>如果颁发者的 时钟比接收者的时钟慢，则当接收者认为 lease 已经过期的时候，颁发者依旧认为 lease 有效。接收 者可以用在 lease 到期前申请新的 lease 的方式解决这个问题。另一方面，如果颁发者的时钟比接收 者的时钟快，则当颁发者认为 lease 已经过期的时候，接收者依旧认为 lease 有效，颁发者可能将 lease 颁发给其他节点，造成承诺失效，影响系统的正确性。</p><p>对于这种时钟不同步，实践中的通常做法是 将颁发者的有效期设置得比接收者的略大，只需大过时钟误差就可以避免对 lease 的有效性的影响</p></blockquote></li><li><p>中心服务器发出的 lease 的含义为:在 lease 的有效期内，中心服务器保证不会修改对应数据的值。</p></li><li><p>节点收到数据和 lease 后，将数据加入本地 Cache，一旦对应的 lease 超时，节点将对应的本地 cache 数据删除。</p></li><li><p>中心服务器在修改数据时，首先阻塞所有新的读请求，并等待之前为该数据发出的所有 lease 超时过期，然后修改数据的值。</p></li></ul><p><strong>基本流程</strong>：</p><p><em>读操作</em></p><ul><li>判断元数据是否已经处于本地 cache 且 lease 处于有效期内， 是就直接返回元数据信息</li><li>否则向中心服务器节点请求读取元数据信息。</li><li>服务器收到读请求后返回数据同时颁发一个对应的lease</li><li>客户端收到服务器返回的数据记录到本地cache，并返回给client</li><li>如果失败或超时，读取失败退出流程。</li></ul><p><em>写操作</em></p><ul><li>节点向服务器发起请求修改元数据</li><li>服务器收到请求，阻塞所有新的读请求，接收请求，但是不返回数据</li><li>服务器等待所有与该元数据相关的lease超时</li><li>服务器修改元数据并向客户端节点返回修改成功</li></ul><p>服务器一旦 发出数据及 lease，无论客户端是否收到，也无论后续客户端是否宕机，也无论后续网络是否正常， 服务器只要等待 lease 超时，就可以保证对应的客户端节点不会再继续 cache 数据，从而可以放心的 修改数据而不会破坏 cache 的一致性。</p><p><strong>存在的问题：</strong></p><ol><li><p>服务器在 修改元数据时首先要阻塞所有新的读请求，造成读服务不可用。这是为了防止发出新的 lease 从而引起 不断有新客户端节点持有 lease 并缓存着数据，形成“活锁”</p><blockquote><p>服务器在进入修 改数据流程后，一旦收到读请求则只返回数据但不颁发 lease。从而造成在修改流程执行的过程中， 客户端可以读到元数据，只是不能缓存元数据</p><p>当进入修改流程，服务器颁发的 lease 有效期限选择为已发出的 lease 的最大有效期限。这样做，客户端可以继续在服务器进入修改 流程后继续缓存元数据，但服务器的等待所有 lease 过期的时间也不会因为颁发新的 lease 而不断延 长</p></blockquote></li><li><p>服务器在修改元数据时需要等待所有的 lease 过期超时，从而造成修改元数据 的操作时延大大增大</p><blockquote><p>可以在元数据修改之前服务端主动通知各个节点放弃lease并清除cache中的数据,如果接收到客户端确认放弃则进行修改，否则等待过期进行修改</p></blockquote></li></ol><p><strong>机制分析</strong></p><ul><li><p>Lease是由颁发者授予的在某一有效期内的承诺。这种承诺的内容非常宽泛，可以是数据的正确性，也可以是某种权限，也可以是某种身份。如在 primary-secondary架构中，给节点颁发lease，只有持有 lease 的节点才具有 primary 身份</p></li><li><p>lease机制有很高的容错能力</p><ul><li>容错网络异常。Lease 颁发过程只依赖于网络可以单向通信。颁发者可以不断重复的向接收者发放相同的lease,一旦lease被接受，那么在有效期内就不依赖于网络通信（即使网络完全中断也不会有影响）</li><li>容错节点宕机。如果颁发者宕机，不会影响lease的正确性，颁发者恢复后可以继续遵守lease的承诺，如果颁发者不能恢复，那么只需等待lease超时即可，并不会破坏lease机制。</li><li>lease机制不依赖于存储，颁发者可以持久化颁发过的lease信息，使得宕机恢复后的lease继续生效</li></ul></li><li><p>有效期选择。使用 lease 确定节点状态时，若 lease 时间过短，有可能造成网络瞬断时节点收不到lease从而引起服务不稳定，若lease时间过长，则一旦某节点宕机异常，需要较大的时间等待lease过期才能发现节点异常。工程中，常选择的 lease 时长是 10 秒级别，这是一个经过验证的经验值，实践中可以作为参考并综合选择合适的时长。</p></li></ul><p><strong>应用：确定节点状态</strong></p><blockquote><p>解决网络分化导致的无法确定节点的状态</p><p>参考chubby 和 zookeeper的设计</p></blockquote><p><img src="http://image.yhzhao.cn/img20210911204043.png"></p><p>在primary-secondary 架构的系统中，有三个节点 A、B、C 互为副本，假设最开始时节点 A为 primary，B、C 为 secondary。节点 Q 如何判断节点 A、B、C 的状态是否正常？</p><ul><li><p>节点 A、B、C 周期性的发送 heart beat 报告自身状态，节点 Q 收到 heart beat后发送一个lease，表示节点 Q 确认了节点 A、B、C 的状态，并允许节点在 lease 有效期内正常工作。</p></li><li><p>节点Q给primary节点A一个特殊的 lease，表示A节点可以作为primary工作。一旦节点Q希望切换新的primary B，则只需等前一个 primary A的lease过期，就可以安全的颁发新的lease给新的primary B节点，并不会出现“双主”问题。</p><blockquote><p>双主问题：假设A和Q之间因为网络中断或者是堵塞，导致没有正常接收到心跳包，Q认为A已经出现异常，而指定B为primary节点。同时告诉集群内其他节点这个消息。如果消息先到达B，则会出现A，B两个primary节点。</p><p>原因在于虽然节点 Q 认为节点 A 异常，但节点 A 自己不认为自己异常，依 旧作为 primary 工作。其问题的本质是由于网络分化造成的系统对于“节点状态”认知的不一致</p></blockquote></li></ul><h5 id="3-Quorum-机制"><a href="#3-Quorum-机制" class="headerlink" title="3. Quorum 机制"></a>3. Quorum 机制</h5><blockquote><p>分布式系统为了提高可用性（Availability），采用了副本备份，比如对于HDFS，默认每块数据存三份。某数据块所在的机器宕机了，就去该数据块副本所在的机器上读取。但是，问题来了，当需要修改数据时，就需要更新所有的副本数据，这样才能保证数据的一致性（Consistency）。因此，就需要在 C(Consistency) 和 A(Availability) 之间权衡。</p></blockquote><p><strong>WARO(Write All Read one)</strong></p><p> WARO是一种简单的副本控制协议，当Client请求向某副本写数据时(更新数据)，只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。两个特点：</p><ul><li><p>写操作很脆弱，因为只要有一个副本更新失败，此次写操作就视为失败了</p></li><li><p>读操作很简单，因为，所有的副本更新成功，才视为更新成功，从而保证所有的副本一致。这样，只需要读任何一个副本上的数据即可。</p></li></ul><p>假设有N个副本，N-1个都宕机了，剩下的那个副本仍能提供读服务；但是只要有一个副本宕机了，写服务就不会成功。</p><p><strong>定义</strong></p><p>​    WARO牺牲了更新服务的可用性，最大程度地增强了读服务的可用性。而Quorum就是更新服务和读服务之间进行一个折中。Quorum机制是“抽屉原理”的一个应用。定义如下：假设有N个副本，更新操作wi 在W个副本中更新成功之后，才认为此次更新操作wi 成功。称 该更新操作为“成功提交的更新操作”，称成功提交的更新操作对应的数据为：“成功提交的数据”。对于读操作而言，至少需要读R个副本才能读到此次更新的数据。其中，W+R&gt;N ，即W和R有重叠。一般，W+R=N+1</p><p><img src="data:image/png;base64,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" alt="img"></p><p>假设系统中有5个副本，W=3，R=3。初始时数据为(V1，V1，V1，V1，V1）–成功提交的版本号为1</p><p>当某次更新操作在3个副本上成功后，就认为此次更新操作成功。数据变成：(V2，V2，V2，V1，V1）–成功提交后，版本号变成2</p><p>因此，最多只需要读3个副本，一定能够读到V2(此次更新成功的数据)。而在后台，可对剩余的V1 同步到V2，而不需要让Client知道。</p><p>Quorum 机制的三个系统参数 N、W、R 控制了系统的可用性，也是系统对用户的服务承诺:数 据最多有 N 个副本，但数据更新成功 W 个副本即返回用户成功。</p><p><strong>机制分析</strong></p><ol><li>Quorum机制无法保证强一致性</li></ol><p>所谓强一致性就是：任何时刻任何用户或节点都可以读到<strong>最近一次成功提交的副本数据</strong>。强一致性是程度最高的一致性要求，也是实践中最难以实现的一致性。</p><p> 因为，仅仅通过Quorum机制无法确定最新已经成功提交的版本号。</p><p>比如，上面的V2 成功提交后（已经写入W=3份），尽管读取3个副本时一定能读到V2，如果刚好读到的是(V2，V2，V2），则此次读取的数据是最新成功提交的数据，因为W=3，而此时刚好读到了3份V2。如果读到的是（V2，V1，V1），则无法确定是一个成功提交的版本，还需要继续再读，直到读到V2的达到3份为止，这时才能确定V2 就是已经成功提交的最新的数据。</p><blockquote><p> 如何读取最新的数据？</p><p>在已经知道最近成功提交的数据版本号的前提下，最多读R个副本就可以读到最新的数据了。</p></blockquote><blockquote><p>如何确定最高版本号的数据是一个成功提交的数据？</p><p>继续读其他的副本，直到读到的 最高版本号副本 出现了W次。</p></blockquote><ol start="2"><li>基于Quorum机制选择 primary</li></ol><p>在 primary-secondary 协议中，当 primary 异常时，需要选择出一个新的 primary，之后 secondary 副本与 primary 同步数据。通常情况下，选择新的 primary 的工作是由某一中心节点完成的，在引入 quorum 机制后，常用的 primary 选择方式与读取数据的方式类似: </p><p><strong>中心节点读取 R 个副本，选择 R 个副本中版本号最高的副本作为新的 primary。新 primary 与至少 W 个副本完成数据同步后作为新 的 primary 提供读写服务。</strong></p><blockquote><p>首先，R 个副本中版本号最高的副本一定蕴含了最新的成功提交的数据。 再者，虽然不能确定最高版本号的数是一个成功提交的数据，但新的 primary 在随后与 secondary 同 步数据，使得该版本的副本个数达到 W，从而使得该版本的数据成为成功提交的数据。</p></blockquote><p>新选出的primary不能立即提供服务，还需要与至少与W个副本完成同步后，才能提供服务</p><blockquote><p>为了保证Quorum机制的规则：W+R&gt;N</p></blockquote><p>至于如何处理同步过程中冲突的数据，则需要视情况而定。</p><p>(V2，V2，V1，V1，V1），R=3，如果读取的3个副本是：(V1，V1，V1)则高版本的 V2需要丢弃。</p><p><img src="http://image.yhzhao.cn/imgimage-20210701010633339.png" alt="image-20210701010633339"></p><p>如果读取的3个副本是（V2，V1，V1），则低版本的V1需要同步到V2</p><p><img src="http://image.yhzhao.cn/imgimage-20210701010646230.png" alt="image-20210701010646230"></p><h5 id="4-工程应用"><a href="#4-工程应用" class="headerlink" title="4. 工程应用"></a>4. 工程应用</h5><p><strong>Zookeeper</strong></p><h4 id="3-3-2-去中心化副本控制协议"><a href="#3-3-2-去中心化副本控制协议" class="headerlink" title="3.3.2 去中心化副本控制协议"></a>3.3.2 去中心化副本控制协议</h4><p>去中心化副本控制协议没有中心节点，协议中所有的节点都是完全对等的，节点之间通过平等协商达到一致。从而去中心化协议没有因为中心化节点异常而带来的停服务等问题。但协议实现比较复杂</p><blockquote><p>Paxos</p><p>Raft</p><p>共识算法与一致性: 共识，节点之间的状态，一致性数据副本之间的状态</p></blockquote><h3 id="3-5-分布式系统设计机制"><a href="#3-5-分布式系统设计机制" class="headerlink" title="3.5 分布式系统设计机制"></a>3.5 分布式系统设计机制</h3><h4 id="重试"><a href="#重试" class="headerlink" title="重试"></a>重试</h4><h4 id="心跳"><a href="#心跳" class="headerlink" title="心跳"></a>心跳</h4><h3 id="3-6-日志机制"><a href="#3-6-日志机制" class="headerlink" title="3.6 日志机制"></a>3.6 日志机制</h3><hr><p>书籍</p><p><a href="http://book.mixu.net/distsys/">Distributed systemsfor fun and profit</a> 【<a href="https://zhuanlan.zhihu.com/p/42234635">翻译</a>】</p><p><strong>参考文章</strong></p><p><a href="https://www.cnblogs.com/xybaby/p/8544715.html">分布式学习最佳实践</a></p><p><a href="http://www.hollischuang.com/archives/681">两阶段提交与三阶段提交</a></p><p><a href="https://blog.csdn.net/qq_18298439/article/details/100293016">分布式系统一致性问题</a></p><p><a href="https://zhuanlan.zhihu.com/p/130974371">分布式一致性协议概述</a></p><p><a href="https://nicky-chin.cn/2018/04/25/cap-base-flp/">分布式之【CAP理论、BASE理论 、FLP不可能定理】</a></p><p><a href="https://www.jianshu.com/p/e3d870ae2059">Lease</a></p><p>raft paxos </p><p> 节点、时钟、网络、失败</p><p> <a href="https://zhuanlan.zhihu.com/p/130974371">MIT课程实验</a>  Raft</p><p><a href="https://blog.csdn.net/reed1991/article/details/104792199?spm=1001.2014.3001.5501">https://blog.csdn.net/reed1991/article/details/104792199?spm=1001.2014.3001.5501</a> kafka的零拷贝技术</p><p><a href="https://blog.csdn.net/reed1991/article/details/102466180?spm=1001.2014.3001.5501">https://blog.csdn.net/reed1991/article/details/102466180?spm=1001.2014.3001.5501</a> Raft 协议</p><p><a href="https://blog.csdn.net/reed1991/article/details/99710714?spm=1001.2014.3001.5501">https://blog.csdn.net/reed1991/article/details/99710714?spm=1001.2014.3001.5501</a> MVCC</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、概念&quot;&gt;&lt;a href=&quot;#一、概念&quot; class=&quot;headerlink&quot; title=&quot;一、概念&quot;&gt;&lt;/a&gt;一、概念&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;分布式要解决什么问题呢&lt;/strong&gt;？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;解决持久化数据太大，单个节点的硬</summary>
      
    
    
    
    <category term="读书笔记" scheme="https://yhzhao.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="分布式" scheme="https://yhzhao.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>大型网站技术架构</title>
    <link href="https://yhzhao.cn/articles/12189.html"/>
    <id>https://yhzhao.cn/articles/12189.html</id>
    <published>2021-09-10T00:46:38.000Z</published>
    <updated>2021-09-14T16:38:00.620Z</updated>
    
    
    
    
    <category term="读书笔记" scheme="https://yhzhao.cn/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="架构" scheme="https://yhzhao.cn/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>博弈论1-发展历史</title>
    <link href="https://yhzhao.cn/articles/60737.html"/>
    <id>https://yhzhao.cn/articles/60737.html</id>
    <published>2021-09-10T00:46:11.000Z</published>
    <updated>2021-09-14T16:38:00.620Z</updated>
    
    
    
    
    <category term="课程笔记" scheme="https://yhzhao.cn/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="博弈论" scheme="https://yhzhao.cn/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>Python SDK 打包</title>
    <link href="https://yhzhao.cn/articles/4435.html"/>
    <id>https://yhzhao.cn/articles/4435.html</id>
    <published>2021-09-10T00:45:18.000Z</published>
    <updated>2021-09-14T16:38:00.616Z</updated>
    
    
    
    
    <category term="计算机语言" scheme="https://yhzhao.cn/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AF%AD%E8%A8%80/"/>
    
    
    <category term="Python" scheme="https://yhzhao.cn/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Sql之花式Join</title>
    <link href="https://yhzhao.cn/articles/31061.html"/>
    <id>https://yhzhao.cn/articles/31061.html</id>
    <published>2021-09-07T22:53:22.000Z</published>
    <updated>2021-09-14T16:38:00.620Z</updated>
    
    
    
    
    <category term="工程实践" scheme="https://yhzhao.cn/categories/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/"/>
    
    
    <category term="sql" scheme="https://yhzhao.cn/tags/sql/"/>
    
    <category term="数据库" scheme="https://yhzhao.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
</feed>
