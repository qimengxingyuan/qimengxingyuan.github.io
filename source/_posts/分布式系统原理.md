---
title: 分布式系统原理
top: false
cover: false
toc: true
mathjax: true
abbrlink: 40164
date: 2021-09-10 00:47:33
password:
summary:
tags:
    - 分布式
categories:
    - 读书笔记
---




# 一、概念

**分布式要解决什么问题呢**？

- 解决持久化数据太大，单个节点的硬盘无法存储的问题

- 解决运算量太大，单个节点的内存、CPU无法处理的问题

解决这些问题，有两种思路：**scale up(纵向扩展) ，scale out(横向扩展)**。前者就是提升单个节点的能力，更大的磁盘，更快的CPU，定制的软硬件，然而这意味着更高的价格，而且再怎么scale up 也是有上限的。后者就是把存储、计算任务分担到普通的机器上，通过动态增加节点来应对数据量的增长，但缺点是多个节点的管理、任务的调度比较复杂，这也是分布式系统研究和解决的问题。

只有当数据量达到单机无法存储、计算的情况下才考虑分布式，不然都是自找麻烦。状态的维护比计算要难很多。分布式系统要做的任务就是把多台机器有机的组合、连接起来，让其协同完成一件任务，可以是计算任务，也可以是存储任务

- 分布式存储
- 分布式计算

> <font color='red'>Q: 为什么大多都会将分布式系统分为存储和计算？或者除了实际完成的功能区别，还有哪些不同</font>
>
> 分布式系统是将在单机完成的存储和计算，分发到多个几点进行处理

由于分布式系统多节点、通过网络通信的拓扑结构，因此会引入很多单机系统没有的问题，为了解决这些问题又会引入更多的机制、协议，带来更多的问题。。。。

## 1. 模型

#### 1.1 **分片（partition）**。

对于计算，那么就是对计算任务进行切换，每个节点算一些，最终汇总就行了，MapReduce的思想；对于存储，每个节点存一部分数据就行了。当数据规模变大的时候，Partition是唯一的选择，同时也会带来一些好处：

- 提升性能和并发，操作被分发到不同的分片，相互独立
- 提升系统的可用性，即使部分分片不能用，其他分片不会受到影响

![](http://image.yhzhao.cn/img20210911204452.png)

理想的情况下，有分片就能解决之前提到的资源不足的问题。

#### 1.2 异常

​	任何在设计阶段考虑到的异常情况一定会在系统实际运行中发生，但在系统实际运行遇到的异常却很有可能在设计时未能考虑，所以，除非需求指标允许，在系统设计时不能放过任何异常情况。

##### 机器宕机

##### 网络异常

> 消息丢失，两片节点之间彼此完全无法通信，即出现了“网络分化”；
>
> 消息乱序，有一定的概率不是按照发送时的顺序依次到达目的节点，考虑使用序列号等机制处理网络消息的乱序问题，使得无效的、过期的网络消息不影响系统的正确性；
>
> 数据错误，数据传输过程中发生了错误。

##### **三态**

> 如果某个节点向另一个节点发起RPC调用，即某个节点A 向另一个节点B 发送一个消息，节点B 根据收到的消息内容完成某些操作，并将操作的结果通过另一个消息返回给节点A，那么这个RPC 执行的结果有三种状态：成功、失败、超时（未知）

##### **存储数据丢失**

>  对于有状态节点来说，数据丢失意味着状态丢失，通常只能从其他节点读取、恢复存储或状态

#### 1.3 **冗余（Replication）**

分布式系统中有大量的节点，且通过网络通信。单个节点的故障（进程crash、断电、磁盘损坏）是个小概率事件，但整个系统的故障率会随节点的增加而指数级增加，网络通信也可能出现断网、高延迟的情况。在这种一定会出现的“异常”情况下，分布式系统还是需要继续稳定的对外提供服务，即需要较强的容错性。

> <font color='red'>Q: TCP 协议是可靠的、面向连接的传输服务，但在分布式系统的协议设计中不能认为所有网络通信都基于TCP 协议则通信就是可靠的？</font>
>
> - TCP协议只能保证同一个TCP 链接内的网络消息不乱序，TCP 链接之间的网络消息顺序则无法保证。
> - 当应用程序通过调用系统函数send发送一个TCP数据流时，函数返回成功，但是这仅仅意味着该消息被本机的网络协议栈所接收。即使数据被对端的网络协议栈所接收，并返回确认包，也仅仅以为这数据到达了对端的协议栈。不能假设对端上层应用已经接收并正确处理。所以消息是可能丢失的。
> - 拜占庭将军

解决方式是由多个节点负责同一个任务，最为常见的就是分布式存储中，多个节点负责存储同一份数据，以此增强可用性与可靠性。同时，冗余也会带来性能的提升，比如数据的本地化可以减少用户的等待时间。

> <font color='red'>Q: 有哪些具体的关于冗余存储带来的可靠性和性能提升的案例？</font>
>
> <font color='red'>Q: 无论是存储还是计算，需要先定位数据再进行操作，相关的解决方案？</font>

![](http://image.yhzhao.cn/img20210911204314.png)

Partitioned是把数据分成相互独立的数据集，这样可以避免数据的增长带来的影响, 同时可以解决单点故障，提高了系统的可用性。Replication意味着数据被完整的拷贝到了多个节点上。比如三备份存储，提高系统的读IO；Replication多数据中心备份，降低网络延迟。

> <font color='red'>Q: 对于Partitioned和Replication来说：一定意义上分片能够解决计算过程中单点故障，冗余解决了存储过程中的单点故障，提高了可用性。但冗余带来的读写性能提高是额外收益吗？两者的关系如何？</font>
>
> 类比磁盘阵列（RAID）的工作原理
>
> <font color='red'>Q: 涉及到的[纠删码](https://blog.csdn.net/u011026968/article/details/52295666)问题</font>

#### 1.4 **一致性（**consistency**）**

Partition和Replication是解决分布式系统问题的一记组合拳，很多具体的问题都在用这个思路去解决。

但这并不是银弹，往往是为了解决一个问题，会引入更多的问题。为了可用性与可靠性保证，引用了冗余。有了冗余，各个**副本间的一致性问题**就变得很头疼，一致性在系统的角度和用户的角度又有不同的等级划分。

- 如果要保证强一致性，那么会影响可用性与性能，在一些应用（比如电商、搜索）是难以接受的。

- 如果是最终一致性，那么就需要处理数据冲突的情况。
>  [CAP](https://zhuanlan.zhihu.com/p/23278877)、[FLP](https://nicky-chen.github.io/2018/04/25/cap-base-flp/)这些理论告诉我们，在分布式系统中，没有最佳的选择，都是需要权衡，做出最合适的选择。牺牲一致性而换取高可用性，是比较常用的方式

##### **强一致性（Strong ）**

当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值。根据 CAP 理论，这种实现需要牺牲可用性。

- 任何一次读都能读到某个数据的最近一次写的数据。
- 系统中的所有进程，看到的操作顺序，都和全局时钟下的顺序一致。

> <font color='red'>Q: 分布式系统的一致性和Mysql的事务一致性是怎样的关系</font>
>
> 两者不是一个概念，mysql的两阶段提交指的是一个记录的完成过程。
>
> ![](http://image.yhzhao.cn/img20210415203516.png)
>
> <img src="http://image.yhzhao.cn/img20210415203602.png" style="zoom:33%;" />
>
> - [知乎回答](https://www.zhihu.com/question/275845393/answer/397349131)
>
> <font color='red'>Q: 强一致性的实现方式，应用场景</font>
>
> 
>
> <font color='red'>Q: 事务一致性和数据一致性</font>
>
> 

###### *两阶段提交*

![](http://image.yhzhao.cn/img20210414215104.png)

![](http://image.yhzhao.cn/img20210414215650.png)

**同步阻塞问题**。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

**单点故障**。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。

**数据不一致**。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。

**二阶段无法解决的问题**当协调者出错，同时参与者也出错时，两阶段无法保证事务执行的完整性。考虑协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。

###### *三阶段提交* 

- 引入超时机制。同时在协调者和参与者中都引入超时机制。（两阶段提交只有协调者存在超时）
- 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的

三阶段提交有`CanCommit`、`PreCommit`、`DoCommit`三个阶段。3PC主要解决的**单点故障问题，并减少阻塞**，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况

```
在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（*一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了*）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ）
```

![](http://image.yhzhao.cn/img20210911204349.png)



>  <font color='red'>Q: 为什么要变成3阶段，如果还是两阶段提交，在参与者那里引入超时是否也能达到相同效果</font>
>
> 其实是为了第三步的可以默认执行，如果收到了pre的消息，说明大家大概率会提交。可以理解为将两阶段提交的第一步做了拆分。

*there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos* 

​																																									--- Mike Burrows

##### **顺序一致性（Sequential）**

- 任何一次读都能读到某个数据的最近一次写的数据。
- 系统的所有进程的顺序一致，而且是合理的。即不需要和全局时钟下的顺序一致，错的话一起错，对的话一起对。

![](http://image.yhzhao.cn/img20210911203755.png)

> <font color='red'>Q: 哪些场景下需要顺序一致性的保证？</font>
>
> 内存模型、Zookeeper的Zab协议

##### **最终一致性（Eventually）**

当用户从异步从库读取时，果此异步从库落后，他可能会看到过时的信息。这种不一致只是一个暂时的状态，如果等待一段时间，从库最终会赶上并与主库保持一致。这称为**最终一致性。**

###### *读写一致性*

​	回复某个帖子然后想马上查看，但刚提交的回复可能尚未到达从库，看起来好像是刚提交的数据丢失了。在这种情况下，我们需要**读写一致性**。它可以保证，如果用户刷新页面，他们总会看到自己刚提交的任何更新。它不会对其他用户的写入做出承诺，其他用户的更新可能稍等才会看到，但它保证用户自己提交的数据能马上被自己看到。-- *读己之所写一致性*

> 如何实现读写一致性？
>
> 1. 最简单的方案，**对于某些特定的内容，都从主库读。**个人主页信息只能由用户本人编辑，而不能由其他人编辑。因此，永远从主库读取用户自己的个人主页，从从库读取其他用户的个人主页。
> 2. 如果应用中的大部分内容都可能被用户编辑的情况下可以使用其他标准来决定是否从主库读取，例如可以**记录每个用户最后一次写入主库的时间**，一分钟内都从主库读，同时**监控从库的最后同步时间**，任何超过一分钟没有更新的从库不响应查询。
> 3. 还有一种更好的方法是，客户端可以**在本地记住最近一次写入的时间戳**，发起请求时带着此时间戳。从库提供任何查询服务前，需确保**该时间戳前的变更都已经同步到了本从库中**。如果当前从库不够新，则可以从另一个从库读，或者等待从库追赶上来。

###### *因果一致性*

​	需要保证因果一致性的场景往往发生在**分区**（也称为**分片**）的分布式数据库中。分区后，每个节点并不包含全部数据。不同的节点独立运行，因此不存在**全局写入顺序。**如果用户A发布一篇文章，用户B提交一个回复。问题写入了节点A，回复写入了节点B。因为同步延迟，发起查询的用户可能会先看到回复，再看到文章。为了防止这种异常，需要**因果一致性**的保证。 即如果一系列写入按某个逻辑顺序发生，那么任何人读取这些写入时，会看见它们以正确的逻辑顺序出现。

>  这是一个实际却很难解决的问题。一种方案是应用保证将问题和对应的回答写入相同的分区。但并不是所有的数据都能如此轻易地判断因果依赖关系。
>
> Todo 向量时钟

###### *单调读*

​	用户从某从库查询到了一条记录，再次刷新后发现此记录不见了。如果用户从不同从库进行多次读取，就可能发生这种情况。**单调读**可以保证这种异常不会发生。单调读意味着如果一个用户进行多次读取时，如果先前读取到较新的数据，后续读取不会得到更旧的数据。**单调读**比强一致性更弱，比最终一致性更强。

> 实现单调读取的一种方式是**确保每个用户总是从同一个节点进行读取**（不同的用户可以从不同的节点读取），比如可以基于用户ID的哈希值来选择节点，而不是随机选择节点。



##### 总结

对于分布式系统的一致性，前述举例都是在分布式存储的应用中。在**分布式计算**中，以上概念有哪些应用和解决方式？



## 2. 分布式系统特性总结

#### 可扩展性

​	分布式系统的根本目标就是为了处理单个计算机无法处理的任务，**当任务增加的时候，分布式系统的处理能力需要随之增加**。要比较方便的通过增加机器来应对数据量的增长，同时，当任务规模缩减的时候，可以撤掉一些多余的机器，达到动态伸缩的效果。好的分布式系统总在追求“线性扩展性”，也就是使得系统的某一指标可以随着集群中的机器数量**线性增长**。

<font color='red'>理想目标：追求线性扩展</font>

为了满足这一特性，涉及到的问题：在动态增加节点的时候，需要进行任务（可能是计算，可能是数据存储）的迁移，以达到动态均衡。如何对任务进行拆分，将任务的子集分配到每一个节点。 

**待解决的问题： *分布式系统分片方式***

#### 可用性

​	系统的可用性(availability)指系统在面对各种异常时可以正确提供服务的能力。系统的可用性可以用系统停服务的时间与正常服务的时间的比例来衡量，也可以用某功能的失败次数与成功次数的比例来衡量。可用性是分布式的重要指标，衡量了系统的鲁棒性，是系统容错能力的体现。

​	分布式系统由大量异构的节点和网络组成，节点可能会crash、断电、磁盘损坏，网络可能丢包、延迟、网络分割。系统的规模放大了出故障的概率，因此分布式系统中，故障是常态。那么分布式系统的其中一个设计目标就是容错，在部分故障的情况下仍然对外提供服务

<font color='red'>追求目标就是7 * 24，即永远在线</font>

![](http://image.yhzhao.cn/img20210911204412.png)

**冗余(副本机制)是提高可用性、可靠性的法宝**。

冗余就是说多个节点负责相同的任务，在需要状态维护的场景，比如分布式存储中使用非常广泛。在分布式计算，如MapReduce中，当一个worker运行异常缓慢时，master会将这个worker上的任务重新调度到其它worker，以提高系统的吞吐，这也算一种冗余。但存储的冗余相比计算而言要复杂许多，因此主要考虑存储的冗余。

*怎么保证并发情况下各个副本数据的一致性，是否有一个节点有决定更新的顺序，这就是中心化、去中心化副本协议的区别*

**待解决的问题： *副本控制协议***



#### 一致性

​	分布式系统为了提高可用性，总是不可避免的使用副本的机制，从而引发副本一致性的问题。越是强的一致的性模型，对于用户使用来说使用起来越简单

​	CAP理论就是说分布式数据存储，最多只能同时满足一致性（C，Consistency）、可用性（A， Availability）、分区容错性（P，Partition Tolerance）中的两者。但一致性和可用性都是一个度的问题，是0到1，而不是只有0和1两个极端

​	对于分布式数据系统：

```
N — 数据复制的份数
W — 更新数据时需要保证写完成的节点数
R — 读取数据的时候需要读取的节点数
    1.如果W+R>N：则是强一致性，写的节点和读的节点重叠。例如对于典型的一主一备同步复制的关系型数据库。N=2,W=2,R=1，则不管读的是主库还是备库的数据，都是一致的。
    2.如果W+R<=N：则是弱一致性。例如对于一主一备异步复制的关系型数据库，N=2,W=1,R=1，则如果读的是备库，就可能无法读取主库已经更新过的数据，所以是弱一致性。
    对于分布式系统，为了保证高可用性，一般设置N>=3。不同的N,W,R组合，是在可用性和一致性之间取一个平衡，以适应不同的应用场景。

如果N=W,R=1，任何一个写节点失效，都会导致写失败，因此可用性会降低，但是由于数据分布的N个节点是同步写入的，因此可以保证强一致性。
如果N=R,W=1，只需要一个节点写入成功即可，写性能和可用性都比较高。但是读取其他节点的进程可能不能获取更新后的数据，因此是弱一致性。这种情况下，如果W<(N+1)/2，并且写入的节点不重叠的话，则会存在写冲突 
```



#### 高性能

- 系统的吞吐能力，指系统在某一时间可以处理的数据总量，通常可以用系统每秒处理的总的数据量来衡量；

- 系统的响应延迟，指系统完成某一功能需要使用的时间；

- 系统的并发能力，指系统可以同时完成某一功能的能力，通常也用QPS(query per second)来衡量。

三个性能指标往往会相互制约，追求高吞吐的系统，往往很难做到低延迟；系统平均响应时间较长时，也很难提高QPS。



## 3. 原理

### 3.1 数据分布方式

​	将一个单机问题使用分布式解决，首先要解决的就是如何将问题拆解为可以使用多机分布式解决，使得分布式系统中的每台机器负责原问题的一个子集。由于无论是计算还是存储，其问题处理的对象都是数据，数据不存在于一台机器或进程中，所以如何拆解分布式系统的输入数据成为分布式系统的基本问题。

#### 哈希取模

​	哈希方式是最常见的数据分布方式，实现方式是通过可以描述记录的业务的id或key(比如用户 id)，通过Hash函数的计算求余。余数作为处理该数据的服务器索引编号处理。![](http://image.yhzhao.cn/img20210911203831.png)



*优点*：只需要通过计算就可以映射出数据和处理节点的关系，不需要存储映射

*缺点*：如果id分布不均匀可能出现计算、存储倾斜的问题，在某个节点上分布过重。并且当处理节点宕机时，这种”硬哈希“的方式会直接导致部分数据异常。

**扩容问题**：如果是”无状态“型的节点，影响比较小，但遇到”有状态“的存储节点需要扩容时，几乎所有的数据需要被迁移并重新分布。工程上的一种解决方式是, 使得集群规模成倍(按2的幂的机器数)扩展，按照数据重新计算哈希。原本一台机器上的数据只需迁移一半到另一台对应的机器上即可完成扩展。![](http://image.yhzhao.cn/img20210911204122.png)

另外一种解决方式是不再简单的将哈希值与机器做除法取模映射，而是将**对应关系作为元数据由专门的元数据服务器管理**。同时，哈希值取模个数往往大于机器个数，这样同一台机器上需要负责多个哈希取模的余数。但需要以较复杂的机制维护大量的元数据

#### 一致性哈希

​	使用一个哈希函数计算数据或数据特征的哈希值，令该哈希函数的输出值域为一个封闭的环，最大值+1=最小值。将节点随机分布到这个环上，每个节点负责处理从自己开始顺时针至下一个节点的全部哈希值域上的数据.<img src="http://image.yhzhao.cn/img20210415004119.png" style="zoom:50%;" /><img src="http://image.yhzhao.cn/img20210415004344.png" style="zoom:50%;" />

*优点*：任意动态添加、删除节点，每次添加、删除一个节点仅影响一致性哈希环上相邻的节点。 为了尽可能均匀的分布节点和数据，一种常见的改进算法是引入虚节点的概念，系统会创建许多虚拟节点，个数远大于当前节点的个数，均匀分布到一致性哈希值域环上。读写数据时，首先通过数据的哈希值在环上找到对应的虚节点，然后查找到对应的real节点。这样在扩容和容错时，大量读写的压力会再次被其他部分节点分摊，主要解决了压力集中的问题。

> 一旦某个节点不可用，该节点将使得多个虚节点不可用，从而使得多个相邻的真实节点负担失效节点的压力。同理，一旦加入一个新节点，可以分配多个虚节点，从而使得新节点可以 负担多个原有节点的压力，从全局看，较容易实现扩容时的负载均衡
>
> 使用一致性哈希的方式需要将节点在一致性哈希环上的位置作为元信息加以管理，这点比直接使用哈希分布数据的方式要复杂。然而，节点的位置信息只于集群中的机器规模相关，且元信息量要小很多

- 为什么元信息量会小很多

#### 数据范围

​	将数据按特征值的值域范围划分为不同的区间，使得集群中每台（组）服务器处理不同区间的数据。为了数据迁移等负载均衡操作的方便，往往利用动态划分区间的技术，使得每个区间中服务的数据量尽量的一样多。当某个区间的数据量较大时，通过将区间“分裂”的方式拆分为两个区间，使得每个数据区间中的数据量都尽量维持在一个较为固定的阈值之下。

​	缺点是由于数据分布信息不能通过计算获取，需要引入一个模块存储这些映射信息。这就增加了模块依赖，可能会有性能和可用性的额外代价。且一般元信息的数据量比较大。

> 哈希分布数据的方式使得系统中的数据类似一个哈希表。按范围分数据的方式则使得从全局看 数据类似一个 B 树。每个具体的服务器都是 B 树的叶子节点，元数据服务器是 B 树的中间节点。
>
> 使用范围分布数据的方式的最大优点就是可以灵活的根据数据量的具体情况拆分原有数据区间， 拆分后的数据区间可以迁移到其他机器，一旦需要集群完成负载均衡时，与哈希方式相比非常灵活。 另外，当集群需要扩容时，可以随意添加机器，而不限为倍增的方式，只需将原机器上的部分数据 分区迁移到新加入的机器上就可以完成集群扩容。
>
> 按范围分布数据方式的缺点是需要维护较为复杂的元信息。随着集群规模的增长，元数据服务 器较为容易成为瓶颈，从而需要较为负责的多元数据服务器机制解决这个问题。

<img src="http://image.yhzhao.cn/img20210415005542.png" style="zoom:33%;" />

> HBase

#### 数据块

​		许多文件系统经常采用类似设计，与具体的数据特征无关，而是将数据视为一个顺序增长的文件，并将这个文件按照某一较为固定的大小划分为若干数据块，将数据块均匀的分布在各个节点，这种做法也需要外部节点来存储映射关系。由于与具体的数据内容无关，按数据量分布数据的方式一般没有数据倾斜的问题，数据总是被均匀切分并分布到集群中。当集群需要重新负载均衡时，只需通过迁移数据块即可完成。集群扩容也没有太大的限制，只需将部分数据库迁移到新加入的机器上即可以完成扩容。

![](http://image.yhzhao.cn/img20210911203921.png)

HDFS

### 3.2 数据副本

​	分布式系统容错、提高可用性的基本手段就是使用副本。对于数据副本的分布方式主要影响系统的可扩展性。一种基本的数据副本策略是以机器为单位，若干机器互为副本，副本机器之间的数据完全相同。这种策略适用于上述各种数据分布方式。其优点是非常简单，其缺点是恢复数据的效率不高、可扩展性也不高。

> 假设有 3 个副 本机器，某时刻其中某台机器磁盘损坏，丢失了全部数据，此时使用新的机器替代故障机器，为了 是的新机器也可以提供服务，需要从正常的两台机器上拷贝数据。此种全盘拷贝数据一般都较为消 耗资源，为了不影响服务质量，实践中往往采用两种方式:
>
> 一、将一台可用的副本机器下线，专门 作为数据源拷贝数据，这样做的缺点是造成实际正常副本数只有 1 个，对数据安全性造成巨大隐患， 且如果服务由于分布式协议设计或压力的要求必须 2 个副本才能正常工作，则该做法完全不可行。
>
> 二、以较低的资源使用限速的方法从两个正常副本上拷贝数据，此方法不停服务，但可以选择服务 压力较小的时段进行。该方法的缺点是速度较慢，如果需要恢复的数据量巨大(例如数 T)，限速 较小(例如 10MB/s)，往往需要数天才能够完成恢复。
>
> mysql实现

​	理想的情况是，若集群有 N 台机器，宕机一台后，该台机器的压力可以均匀分散到剩 下的 N-1 台机器上，每台机器的压力仅仅增加 1/N-1。更合适的做法不是以机器作为副本单位，而是**将数据拆为较合理的数据段，以数据段为单位作为副本**。实践中，常常使得每个数据段的大小尽量相等且控制在一定的大小以内。数据段有很多不同的称谓，segment，fragment，chunk，partition 等等。数据段的选择与数据分布方式直接相关。对于哈希分数据的方式，每个哈希分桶后的余数可以作为一个数据段，为了控制数据段的大小，常常使得分桶个数大于集群规模。一旦将数据分为数据段，则可以以数据段为单位管理副本，从而副本与机器不再硬相关，每台机器都可以负责一定数据段的副本。

<img src="http://image.yhzhao.cn/img20210415010244.png" style="zoom:50%;" />

*优点：*

​	数据丢失后的恢复效率将非常高

> 一旦某台机器的数据丢失，其上数据段的副本将分布在整个集群的所有机器中，而不是仅在几个副本机器中，从而可以从整个集群同时拷贝恢复数据，而集群中每台数据源机器都可以以非常低的资源做拷贝。作为恢复数据源的机器即使都限速1MB/s，若有100 台机器参与恢复，恢复速度也能达到100MB/s

​	容错性能以及集群扩展较好

> 如果出现机器宕机，由于宕机机器上的副本分散于整个集群，其压力也自然分散到整个集群。集群扩展时，设集群规模 为N 台机器，当加入一台新的机器时，只需从各台机器上迁移1/N – 1/N+1 比例的数据段到新机器即实现了新的负载均衡

工程中，完全按照数据段建立副本会引起需要管理的元数据的开销增大，副本维护的难度也相应增大。一种折中的做法是将某些数据段组成一个数据段分组，按数据段分组为粒度进行副本管理。这样做可以将**副本粒度控制**在一个较为合适的范围内。

> Kafka的实现

#### 方式选择

组合拳：数据分段弥补哈希方式带来的数据倾斜问题

> <font color='red'>Q：一些经典的分布式系统的实现方式</font>

<img src="http://image.yhzhao.cn/img20210415125234.png" style="zoom:35%;" />

### 3.3副本控制协议

​	副本控制协议指按特定的协议流程控制副本数据的读写行为，使得副本满足一定的可用性和一致性要求的分布式协议。但要设计一种满足强一致性，且在出现任何网络异常时都可用的副本协议是不可能的。

“中心化(centralized)副本控制协议”和“去中心化(decentralized) 副本控制协议”

>  <font color='red'>Q：对于分布式计算来说副本控制协议有哪些作用？</font>

#### 3.3.1 中心化副本控制协议

​	由一个中心节点协调副本数据的更新、维护副本之间的一致性。比如Primary-secondary协议。

<img src="http://image.yhzhao.cn/img20210415172502.png" style="zoom:60%;" />

优点：

- 协议相对较为简单，所有的副本相关的控制交由中心节点完成。从而使得一个分布式并发控制问题，简化为一个单机并发控制问题

  > 多个节点同时需要修改副本数据时，需要解决“写写”、“读写”等并发冲突。单机系统上常用加锁等方式进行并发控制。对于分布式并发控制，加锁也是一个常用的方法，但如果没有中心节点统一进行锁管理，就需要完全分布式化的锁系统，会使得协议非常复杂

缺点

- 系统的可用性依赖于中心化节点，当中心节点异常或与中心节点通信中断时，系统将失去某些服务。常表现为无法更新

##### 1. Primary-secondary协议

Primary-secondary协议中副本被分为两大类，其中仅有一个副本作为primary副本，其他为secondary副本。维护 primary 副本的节点作为中心节点，中心节点负 责维护数据的更新、并发控制、协调副本的一致性。其协议需要解决四大问题：数据更新流程、数据读取方式、Primary副本的确定和切换、数据同步

> 两阶段提交的一个实现？



**数据更新流程**

1. 数据更新都由primary 节点协调完成。
2. 外部节点将更新操作发给primary 节点
3. primary 节点进行并发控制即确定并发更新操作的先后顺序
4. primary 节点将更新操作发送给secondary 节点
5. primary 根据secondary 节点的完成情况决定更新是否成功并将结果返回外部节点

<img src="http://image.yhzhao.cn/img20210415173500.png" style="zoom:50%;" />



> 如果由primary 直接同时发送给其他N 个副本发送数据，则每个 **secondary 的更新吞吐受限于primary 总的出口网络带宽**，最大为primary 网络出口带宽的1/N。为了解决这个问题，有些系统使用接力的方式同步数据，即primary 将更新发送给第一 个secondary 副本，第一个secondary 副本发送给第二secondary 副本，依次类推。

异常：如果在第四步出现了网络异常如何处理？Quorum机制

应该要和系统的一致性要求相关，如果是最终一致性的实现。可以允许失败，后续会慢慢更新到相应的节点。第五步的操作强依赖与第四步的要求。



**数据读取**

数据读取方式与一致性高度相关。

- 最终一致性，则读取任何副本都可以满足需求。
- 单调读一致性，则可以为副本设置版本号，每次更新后递增版本号，用户读取副本时验证版本号，从而保证用户读到的数据在会话范围内单调递增

- 强一致性，比较困难，有下面几种实现思路

  - 只读primary 副本。secondary 副本将不提供读服务。

    > 实践中，如果副本不与机器绑定，而是按照数据段为单位维护副本，仅有primary 副本提供读服务在很多场景下并不会造出机器资源浪费，假设primary 也是随机的确定的，那么每台机器上都有一些数据的primary 副本，也有另一些数据段的secondary 副本。从而某台服务器实际都提供读写服务
    >
    > *元信息需要维护*

  - 由primary 控制节点secondary 节点的可用性。当primary 更新某个secondary 副本不成功时，primary 将该secondary 副本标记为不可用，从而用户不再读取该不可用的副本。不可用的 secondary 副本可以继续尝试与primary 同步数据，当与primary 完成数据同步后，primary 可以副本标记为可用。

    > 这种方式使得所有的可用的副本，无论是primary 还是secondary 都是可读的，且在一个确定的时间内，某secondary 副本要么更新到与primary 一致的最新状态，要么被标记为不可用，从而符合较高的一致性要求。这种方式依赖于一个中心元数据管理系统，用于记录哪些副本可用，哪些副本不可用。该方式通过降低系统的可用性来提高系统的一致性。



**primary 副本的确定与切换**

​	在primary-secondary 类型的分布式系统中，哪个副本是primary 这一信息都属于元信息，由专门的元数据服务器维护。执行更新操作时，首先查询元数据服务器获取副本的primary 信息，从而进一步执行数据更新流程。在原primary 副本所在机器出现宕机等异常时，可能会导致服务停服。

- 如何确定节点的状态以发现原 primary 节点异常，异常探测。lease

- 换 primary后，不能影响副本的一致性。Quorum

  > 切换的新 primary的副本数据必须与原primary的副本一致。然而在原 primary 已经发送宕机等异常时，如何确定一个secondary 副本使得该副本上的数据与原primary 一致又成为新的问题。就变成了如何确定读取最新数据的问题。



**数据同步**

不一致的secondary 副本需要与primary 进行同步。

通常不一致的形式有三种：

- 由于网络分化等异常，secondary 上的数据落后于primary 上的数据。

  > 回放primary 上的redo 日志

- 在某些协议下，secondary 上的数据有可能是脏数据，需要被丢弃。

  > 直接丢弃有脏数据的副本，这样相当于副本没有数据。
  > 
  >设计一些基于 undo 日志的方式从而可以删除脏数据。
  > 
  >*如何做到的？日志机制*
  
- secondary 是一个新增加的副本，完全没有数据，需要从其他副本上拷贝数据。

  > 直接拷贝 primary 副本的数据，这种方法往往比回放日志追更新进度的方法快很多。但拷贝数据时 primary 副本需要能够继续提供更新服务，这就要求 primary 副本支持快照(snapshot)功能。即对某一刻的副本数据形成快照，然后拷贝快照，拷贝完成后使用回放日志的方式追回快照形成后的更新操作。



##### 2. Lease 机制

> 1989年斯坦福大学的Cary G. Gray和David R. Cheriton
>
> 如何确定异常节点的状态

租约机制，是一种在分布式系统常用的协议，是维护分布式系统数据一致性的一种常用工具。 Lease机制有以下几个特点： 

1. Lease是颁发者对一段时间内数据一致性的承诺； 
2. 颁发者发出Lease后，不管是否被接收，只要Lease不过期，颁发者都会按照协议遵守承诺；
3.  Lease的持有者只能在Lease的有效期内使用承诺，一旦Lease超时，持有者需要放弃执行，重新申请Lease。

当有更改请求时，服务器修改了数据，但是缓存却还没来得及修改，就带来了数据一致性的问题。同时系统要能最大可能的处理节点宕机、网络中断等 异常，最大程度的提高系统的可用性。



**基本原理：**

- 中心服务器在向各节点发送数据时同时向节点颁发一个 lease。每个 lease 具有一个有效期，和信用卡上的有效期类似，lease 上的有效期通常是一个明确的时间点，一旦真实时间超过这个时间点，则 lease 过期失效。

- lease 的有效期与节点收到 lease 的时间无关，节点可能收到 lease 时该 lease 就已经过期失效。 

  > 如何保证时钟同步?
  >
  > 如果颁发者的 时钟比接收者的时钟慢，则当接收者认为 lease 已经过期的时候，颁发者依旧认为 lease 有效。接收 者可以用在 lease 到期前申请新的 lease 的方式解决这个问题。另一方面，如果颁发者的时钟比接收 者的时钟快，则当颁发者认为 lease 已经过期的时候，接收者依旧认为 lease 有效，颁发者可能将 lease 颁发给其他节点，造成承诺失效，影响系统的正确性。
  >
  > 对于这种时钟不同步，实践中的通常做法是 将颁发者的有效期设置得比接收者的略大，只需大过时钟误差就可以避免对 lease 的有效性的影响

- 中心服务器发出的 lease 的含义为:在 lease 的有效期内，中心服务器保证不会修改对应数据的值。

- 节点收到数据和 lease 后，将数据加入本地 Cache，一旦对应的 lease 超时，节点将对应的本地 cache 数据删除。

- 中心服务器在修改数据时，首先阻塞所有新的读请求，并等待之前为该数据发出的所有 lease 超时过期，然后修改数据的值。



**基本流程**：

*读操作*

- 判断元数据是否已经处于本地 cache 且 lease 处于有效期内， 是就直接返回元数据信息
- 否则向中心服务器节点请求读取元数据信息。
- 服务器收到读请求后返回数据同时颁发一个对应的lease
- 客户端收到服务器返回的数据记录到本地cache，并返回给client
- 如果失败或超时，读取失败退出流程。

*写操作*

- 节点向服务器发起请求修改元数据
- 服务器收到请求，阻塞所有新的读请求，接收请求，但是不返回数据
- 服务器等待所有与该元数据相关的lease超时
- 服务器修改元数据并向客户端节点返回修改成功

服务器一旦 发出数据及 lease，无论客户端是否收到，也无论后续客户端是否宕机，也无论后续网络是否正常， 服务器只要等待 lease 超时，就可以保证对应的客户端节点不会再继续 cache 数据，从而可以放心的 修改数据而不会破坏 cache 的一致性。



**存在的问题：**

1. 服务器在 修改元数据时首先要阻塞所有新的读请求，造成读服务不可用。这是为了防止发出新的 lease 从而引起 不断有新客户端节点持有 lease 并缓存着数据，形成“活锁”

   > 服务器在进入修 改数据流程后，一旦收到读请求则只返回数据但不颁发 lease。从而造成在修改流程执行的过程中， 客户端可以读到元数据，只是不能缓存元数据
   >
   > 当进入修改流程，服务器颁发的 lease 有效期限选择为已发出的 lease 的最大有效期限。这样做，客户端可以继续在服务器进入修改 流程后继续缓存元数据，但服务器的等待所有 lease 过期的时间也不会因为颁发新的 lease 而不断延 长

2. 服务器在修改元数据时需要等待所有的 lease 过期超时，从而造成修改元数据 的操作时延大大增大

   > 可以在元数据修改之前服务端主动通知各个节点放弃lease并清除cache中的数据,如果接收到客户端确认放弃则进行修改，否则等待过期进行修改

**机制分析**

- Lease是由颁发者授予的在某一有效期内的承诺。这种承诺的内容非常宽泛，可以是数据的正确性，也可以是某种权限，也可以是某种身份。如在 primary-secondary架构中，给节点颁发lease，只有持有 lease 的节点才具有 primary 身份

- lease机制有很高的容错能力
  - 容错网络异常。Lease 颁发过程只依赖于网络可以单向通信。颁发者可以不断重复的向接收者发放相同的lease,一旦lease被接受，那么在有效期内就不依赖于网络通信（即使网络完全中断也不会有影响）
  - 容错节点宕机。如果颁发者宕机，不会影响lease的正确性，颁发者恢复后可以继续遵守lease的承诺，如果颁发者不能恢复，那么只需等待lease超时即可，并不会破坏lease机制。
  - lease机制不依赖于存储，颁发者可以持久化颁发过的lease信息，使得宕机恢复后的lease继续生效
- 有效期选择。使用 lease 确定节点状态时，若 lease 时间过短，有可能造成网络瞬断时节点收不到lease从而引起服务不稳定，若lease时间过长，则一旦某节点宕机异常，需要较大的时间等待lease过期才能发现节点异常。工程中，常选择的 lease 时长是 10 秒级别，这是一个经过验证的经验值，实践中可以作为参考并综合选择合适的时长。



**应用：确定节点状态**

> 解决网络分化导致的无法确定节点的状态
>
> 参考chubby 和 zookeeper的设计

![](http://image.yhzhao.cn/img20210911204043.png)

在primary-secondary 架构的系统中，有三个节点 A、B、C 互为副本，假设最开始时节点 A为 primary，B、C 为 secondary。节点 Q 如何判断节点 A、B、C 的状态是否正常？

- 节点 A、B、C 周期性的发送 heart beat 报告自身状态，节点 Q 收到 heart beat后发送一个lease，表示节点 Q 确认了节点 A、B、C 的状态，并允许节点在 lease 有效期内正常工作。

- 节点Q给primary节点A一个特殊的 lease，表示A节点可以作为primary工作。一旦节点Q希望切换新的primary B，则只需等前一个 primary A的lease过期，就可以安全的颁发新的lease给新的primary B节点，并不会出现“双主”问题。

  > 双主问题：假设A和Q之间因为网络中断或者是堵塞，导致没有正常接收到心跳包，Q认为A已经出现异常，而指定B为primary节点。同时告诉集群内其他节点这个消息。如果消息先到达B，则会出现A，B两个primary节点。
  >
  > 原因在于虽然节点 Q 认为节点 A 异常，但节点 A 自己不认为自己异常，依 旧作为 primary 工作。其问题的本质是由于网络分化造成的系统对于“节点状态”认知的不一致

#####  3. Quorum 机制

> 分布式系统为了提高可用性（Availability），采用了副本备份，比如对于HDFS，默认每块数据存三份。某数据块所在的机器宕机了，就去该数据块副本所在的机器上读取。但是，问题来了，当需要修改数据时，就需要更新所有的副本数据，这样才能保证数据的一致性（Consistency）。因此，就需要在 C(Consistency) 和 A(Availability) 之间权衡。

**WARO(Write All Read one)**

 WARO是一种简单的副本控制协议，当Client请求向某副本写数据时(更新数据)，只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。两个特点：

- 写操作很脆弱，因为只要有一个副本更新失败，此次写操作就视为失败了

- 读操作很简单，因为，所有的副本更新成功，才视为更新成功，从而保证所有的副本一致。这样，只需要读任何一个副本上的数据即可。

假设有N个副本，N-1个都宕机了，剩下的那个副本仍能提供读服务；但是只要有一个副本宕机了，写服务就不会成功。



**定义**

​	WARO牺牲了更新服务的可用性，最大程度地增强了读服务的可用性。而Quorum就是更新服务和读服务之间进行一个折中。Quorum机制是“抽屉原理”的一个应用。定义如下：假设有N个副本，更新操作wi 在W个副本中更新成功之后，才认为此次更新操作wi 成功。称 该更新操作为“成功提交的更新操作”，称成功提交的更新操作对应的数据为：“成功提交的数据”。对于读操作而言，至少需要读R个副本才能读到此次更新的数据。其中，W+R>N ，即W和R有重叠。一般，W+R=N+1

![img](data:image/png;base64,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)

假设系统中有5个副本，W=3，R=3。初始时数据为(V1，V1，V1，V1，V1）--成功提交的版本号为1

当某次更新操作在3个副本上成功后，就认为此次更新操作成功。数据变成：(V2，V2，V2，V1，V1）--成功提交后，版本号变成2

因此，最多只需要读3个副本，一定能够读到V2(此次更新成功的数据)。而在后台，可对剩余的V1 同步到V2，而不需要让Client知道。

Quorum 机制的三个系统参数 N、W、R 控制了系统的可用性，也是系统对用户的服务承诺:数 据最多有 N 个副本，但数据更新成功 W 个副本即返回用户成功。



**机制分析**

1. Quorum机制无法保证强一致性

所谓强一致性就是：任何时刻任何用户或节点都可以读到**最近一次成功提交的副本数据**。强一致性是程度最高的一致性要求，也是实践中最难以实现的一致性。

 因为，仅仅通过Quorum机制无法确定最新已经成功提交的版本号。

比如，上面的V2 成功提交后（已经写入W=3份），尽管读取3个副本时一定能读到V2，如果刚好读到的是(V2，V2，V2），则此次读取的数据是最新成功提交的数据，因为W=3，而此时刚好读到了3份V2。如果读到的是（V2，V1，V1），则无法确定是一个成功提交的版本，还需要继续再读，直到读到V2的达到3份为止，这时才能确定V2 就是已经成功提交的最新的数据。

>  如何读取最新的数据？
>
> 在已经知道最近成功提交的数据版本号的前提下，最多读R个副本就可以读到最新的数据了。

> 如何确定最高版本号的数据是一个成功提交的数据？
>
> 继续读其他的副本，直到读到的 最高版本号副本 出现了W次。

2. 基于Quorum机制选择 primary

在 primary-secondary 协议中，当 primary 异常时，需要选择出一个新的 primary，之后 secondary 副本与 primary 同步数据。通常情况下，选择新的 primary 的工作是由某一中心节点完成的，在引入 quorum 机制后，常用的 primary 选择方式与读取数据的方式类似: 

**中心节点读取 R 个副本，选择 R 个副本中版本号最高的副本作为新的 primary。新 primary 与至少 W 个副本完成数据同步后作为新 的 primary 提供读写服务。**

> 首先，R 个副本中版本号最高的副本一定蕴含了最新的成功提交的数据。 再者，虽然不能确定最高版本号的数是一个成功提交的数据，但新的 primary 在随后与 secondary 同 步数据，使得该版本的副本个数达到 W，从而使得该版本的数据成为成功提交的数据。

新选出的primary不能立即提供服务，还需要与至少与W个副本完成同步后，才能提供服务

> 为了保证Quorum机制的规则：W+R>N

至于如何处理同步过程中冲突的数据，则需要视情况而定。

(V2，V2，V1，V1，V1），R=3，如果读取的3个副本是：(V1，V1，V1)则高版本的 V2需要丢弃。

![image-20210701010633339](http://image.yhzhao.cn/imgimage-20210701010633339.png)

如果读取的3个副本是（V2，V1，V1），则低版本的V1需要同步到V2

![image-20210701010646230](http://image.yhzhao.cn/imgimage-20210701010646230.png)



##### 4. 工程应用

**Zookeeper**











#### 3.3.2 去中心化副本控制协议

去中心化副本控制协议没有中心节点，协议中所有的节点都是完全对等的，节点之间通过平等协商达到一致。从而去中心化协议没有因为中心化节点异常而带来的停服务等问题。但协议实现比较复杂

> Paxos
>
> Raft
>
> 共识算法与一致性: 共识，节点之间的状态，一致性数据副本之间的状态



### 3.5 分布式系统设计机制

#### 重试



#### 心跳



### 3.6 日志机制









***

书籍

[Distributed systemsfor fun and profit](http://book.mixu.net/distsys/) 【[翻译](https://zhuanlan.zhihu.com/p/42234635)】





**参考文章**

[分布式学习最佳实践](https://www.cnblogs.com/xybaby/p/8544715.html)

[两阶段提交与三阶段提交](http://www.hollischuang.com/archives/681)

[分布式系统一致性问题](https://blog.csdn.net/qq_18298439/article/details/100293016)

[分布式一致性协议概述](https://zhuanlan.zhihu.com/p/130974371)

[分布式之【CAP理论、BASE理论 、FLP不可能定理】](https://nicky-chin.cn/2018/04/25/cap-base-flp/)

[Lease](https://www.jianshu.com/p/e3d870ae2059)

raft paxos 

 节点、时钟、网络、失败



 [MIT课程实验](https://zhuanlan.zhihu.com/p/130974371)  Raft

https://blog.csdn.net/reed1991/article/details/104792199?spm=1001.2014.3001.5501 kafka的零拷贝技术

https://blog.csdn.net/reed1991/article/details/102466180?spm=1001.2014.3001.5501 Raft 协议

https://blog.csdn.net/reed1991/article/details/99710714?spm=1001.2014.3001.5501 MVCC


